{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# &#x1F50D; Checking segmentation outputs from Organelle-Segmenter-Plugin\n",
    "\n",
    "### &#x1F4D6; **How to:** \n",
    "\n",
    "Advance through each block of code sequentially by pressing `Shift`+`Enter`.\n",
    "\n",
    "If a block of code contains &#x1F53D; follow the written instructions to fill in the blanks below that line before running it.\n",
    "```python\n",
    "#### USER INPUT REQUIRED ###\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "## **Final Workflow *(Quality Check)***\n",
    "\n",
    "### summary of steps\n",
    "\n",
    "**QUALITY CHECK OF SEGMENTATIONS**\n",
    "\n",
    "- **`0`** - Establish data and output paths *(preliminary step)*\n",
    "\n",
    "- **`1`** - Import organelle and region segmentations\n",
    "\n",
    "**EDITING SEGMENTATIONS**\n",
    "\n",
    "- **`2`** - Edit/Review individual segmentations *(optional)*\n",
    "\n",
    "**SAVE ALL CORRECT SEGMENTATIONS** - into one folder for quantification\n",
    "\n",
    "- **`3`** - Save organelle and region segmentations into specified folder\n",
    "\n",
    "**EXECUTE QUANTIFICATION**\n",
    "\n",
    "- Define prototype `_batch_process_quantification` function\n",
    "- Run prototype `_batch_process_quantification` function\n",
    "- Define prototype `_batch_summary_stats` function\n",
    "- Run prototype `_batch_summary_stats` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________\n",
    "## \t**IMPORTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &#x1F3C3; **Run code; no user input required**\n",
    "\n",
    "&#x1F453; **FYI:** This code block loads all of the necessary python packages and functions you will need for this notebook. Additionally, a [Napari](https://napari.org/stable/) window will open; this is where you will be able to visual the segmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top level imports\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "from typing import Optional, Union, Dict, List\n",
    "import itertools \n",
    "import glob\n",
    "\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import napari\n",
    "\n",
    "### import local python functions in ../infer_subc\n",
    "sys.path.append(os.path.abspath((os.path.join(os.getcwd(), '..'))))\n",
    "\n",
    "from infer_subc.core.file_io import (read_czi_image,\n",
    "                                        export_inferred_organelle,\n",
    "                                        import_inferred_organelle,\n",
    "                                        export_tiff,\n",
    "                                        list_image_files,\n",
    "                                        read_tiff_image)\n",
    "\n",
    "\n",
    "\n",
    "from infer_subc.constants import *\n",
    "from infer_subc.utils.stats import *\n",
    "from infer_subc.utils.stats_helpers import *\n",
    "from infer_subc.utils.stats import _assert_uint16_labels\n",
    "from infer_subc.core.img import label_uint16\n",
    "\n",
    "\n",
    "import time\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "viewer = napari.Viewer()\n",
    "\n",
    "# TODO: include the file_type option in the main import_inferred_organelle() function\n",
    "def _import_inferred_organelle(name: str, meta_dict: Dict, out_data_path: Path, file_type: str) -> Union[np.ndarray, None]:\n",
    "    \"\"\"\n",
    "    read inferred organelle from ome.tif file\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    name: str\n",
    "        name of organelle.  i.e. nuc, lyso, etc.\n",
    "    meta_dict:\n",
    "        dictionary of meta-data (ome) from original file\n",
    "    out_data_path:\n",
    "        Path object of directory where tiffs are read from\n",
    "    file_type: \n",
    "        The type of file you want to import as a string (ex - \".tif\", \".tiff\", \".czi\", etc.)\n",
    "\n",
    "    Returns\n",
    "    -------------\n",
    "    exported file name\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # copy the original file name to meta\n",
    "    img_name = Path(meta_dict[\"file_name\"])  #\n",
    "    # add params to metadata\n",
    "    if name is None:\n",
    "        pass\n",
    "    else:\n",
    "        organelle_fname = f\"{img_name.stem}-{name}{file_type}\"\n",
    "\n",
    "        organelle_path = out_data_path / organelle_fname\n",
    "\n",
    "        if Path.exists(organelle_path):\n",
    "            # organelle_obj, _meta_dict = read_ome_image(organelle_path)\n",
    "            organelle_obj = read_tiff_image(organelle_path)  # .squeeze()\n",
    "            print(f\"loaded  inferred {len(organelle_obj.shape)}D `{name}`  from {out_data_path} \")\n",
    "            return organelle_obj\n",
    "        else:\n",
    "            print(f\"`{name}` object not found: {organelle_path}\")\n",
    "            raise FileNotFoundError(f\"`{name}` object not found: {organelle_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________\n",
    "# ***QUALITY CHECK OF SEGMENTATIONS***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`0` - Establish data and output paths *(preliminary step)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &#x1F6D1; &#x270D; **User Input Required:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### USER INPUT REQUIRED ###\n",
    "# Copy and paste the paths to the folders where your data is saved inside the quotation marks below. \n",
    "# If you have more than one segmentation data folder, include it in the segmentation_data_2 line. If not, type None wihtout quotation marks\n",
    "# NOTE: for windows, use \"/\" \n",
    "raw_data = \"D:/Experiments (C2-117 - current)/C2-121/C2-121_deconvolution\"\n",
    "segmentation_data = \"D:/Experiments (C2-117 - current)/C2-121/20230921_C2-121_3D-analysis/20230921_C2-121_segmentation\"\n",
    "\n",
    "location_tosave_edited_segmentations = \"D:/Experiments (C2-117 - current)/C2-121/20230921_C2-121_3D-analysis/20240102_C2-121_segmentation-edits\"\n",
    "location_tosave_fullset_gooddata = \"D:/Experiments (C2-117 - current)/C2-121/20230921_C2-121_3D-analysis/C2-121_good-segs\"\n",
    "\n",
    "# In quotation marks, include the extension of the file type for your SEGMENTATION and RAW images\n",
    "raw_file_type = \".tiff\"\n",
    "seg_file_type = \".tiff\"\n",
    "\n",
    "# In quotation marks, write the suffix associated to each segmentation file. If you don't have that image \n",
    "mask_suffix = \"masks_A\"\n",
    "lyso_suffix = \"lyso\"\n",
    "mito_suffix = \"mito\"\n",
    "golgi_suffix = \"golgi\"\n",
    "perox_suffix = \"perox\"\n",
    "ER_suffix = \"ER\"\n",
    "LD_suffix = \"LD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Optional - USER INPUT REQUIRED ###\n",
    "# If your segmentations are saved in more than one folder, fill in the information below about the second file location. If not, type None wihtout quotation marks in all of the lines below.\n",
    "# Copy and paste the paths to the folders where your data is saved inside the quotation marks below. \n",
    "segmentation_data_2 = None\n",
    "\n",
    "# In quotation marks, write the suffix associated to each segmentation file; if \n",
    "mask_suffix_2 = None\n",
    "lyso_suffix_2 = None\n",
    "mito_suffix_2 = None\n",
    "golgi_suffix_2 = None\n",
    "perox_suffix_2 = None\n",
    "ER_suffix_2 = None\n",
    "LD_suffix_2 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#x1F3C3; **Run code; no user input required**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_file_list = list_image_files(Path(raw_data),\".tiff\")\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame({\"Image Name\":raw_file_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &#x1F6D1; &#x270D; **User Input required:**\n",
    "&#x1F53C; Use the list  above to determine the index of the image you would like to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### USER INPUT REQUIRED ###\n",
    "# Utilizing the list above as reference, change this index number (left column in table) to select a specific image\n",
    "num = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`1` - Import organelle and region segmentations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#x1F3C3; **Run code; no user input required**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_img_data, raw_meta_dict = read_czi_image(raw_file_list[num])\n",
    "print(\"Image name:\")\n",
    "print(raw_meta_dict['name'][0].split(\" :: \")[0])\n",
    "\n",
    "mask_seg = _import_inferred_organelle(mask_suffix, raw_meta_dict, Path(segmentation_data), seg_file_type)\n",
    "lyso_seg = _import_inferred_organelle(lyso_suffix, raw_meta_dict, Path(segmentation_data), seg_file_type)\n",
    "mito_seg = _import_inferred_organelle(mito_suffix, raw_meta_dict, Path(segmentation_data), seg_file_type)\n",
    "golgi_seg = _import_inferred_organelle(golgi_suffix, raw_meta_dict, Path(segmentation_data), seg_file_type)\n",
    "perox_seg = _import_inferred_organelle(perox_suffix, raw_meta_dict, Path(segmentation_data), seg_file_type)\n",
    "ER_seg = _import_inferred_organelle(ER_suffix, raw_meta_dict, Path(segmentation_data), seg_file_type)\n",
    "LD_seg = _import_inferred_organelle(LD_suffix, raw_meta_dict, Path(segmentation_data), seg_file_type)\n",
    "\n",
    "if segmentation_data_2 is not None:\n",
    "    mask_seg = _import_inferred_organelle(mask_suffix, raw_meta_dict, Path(segmentation_data_2), seg_file_type)\n",
    "    lyso_seg = _import_inferred_organelle(lyso_suffix, raw_meta_dict, Path(segmentation_data_2), seg_file_type)\n",
    "    mito_seg = _import_inferred_organelle(mito_suffix, raw_meta_dict, Path(segmentation_data_2), seg_file_type)\n",
    "    golgi_seg = _import_inferred_organelle(golgi_suffix, raw_meta_dict, Path(segmentation_data_2), seg_file_type)\n",
    "    perox_seg = _import_inferred_organelle(perox_suffix, raw_meta_dict, Path(segmentation_data_2), seg_file_type)\n",
    "    ER_seg = _import_inferred_organelle(ER_suffix, raw_meta_dict, Path(segmentation_data_2), seg_file_type)\n",
    "    LD_seg = _import_inferred_organelle(LD_suffix, raw_meta_dict, Path(segmentation_data_2), seg_file_type)\n",
    "\n",
    "viewer.layers.clear()\n",
    "viewer.add_image(raw_img_data[0], name='LD_raw', blending='additive')\n",
    "viewer.add_image(LD_seg, opacity=0.3, colormap='magenta')\n",
    "viewer.add_image(raw_img_data[1], name='ER_raw', blending='additive')\n",
    "viewer.add_image(ER_seg, opacity=0.3, colormap='red')\n",
    "viewer.add_image(raw_img_data[2], name='GL_raw', blending='additive')\n",
    "viewer.add_image(golgi_seg, opacity=0.3, colormap='yellow')\n",
    "viewer.add_image(raw_img_data[3], name='LS_raw', blending='additive')\n",
    "viewer.add_image(lyso_seg, opacity=0.3, colormap='cyan')\n",
    "viewer.add_image(raw_img_data[4], name='MT_raw', blending='additive')\n",
    "viewer.add_image(mito_seg, opacity=0.3, colormap='green')\n",
    "viewer.add_image(raw_img_data[5], name='PO_raw', blending='additive')\n",
    "viewer.add_image(perox_seg, opacity=0.3, colormap='bop orange')\n",
    "viewer.add_image(mask_seg, opacity=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#x1F6D1; **STOP: Use the `Napari` window to review all of the segmentations for this image.** &#x1F50E;\n",
    "\n",
    "> ###### **At this point, take note of which segmentations need to be edited, if any. Once you are finished reviewing the images, continue on to the next sections to 1) Edit the segmentation (if necessary) or 2) Save the final set of segmentations for this image in a new folder. This will make preparing for quantitative analysis much simpler.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________\n",
    "# ***EDITING SEGMENTATIONS***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`2` - Edit/Review individual segmentations *(optional)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &#x1F6D1; &#x270D; **User Input:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### USER INPUT REQUIRED ###\n",
    "# Indicate which segmentations need editing by typing True. If the segmentations are good and do not need editing, indicate False.\n",
    "edit_cell = False\n",
    "edit_nuc = False\n",
    "edit_LD = False \n",
    "edit_ER = False\n",
    "edit_golgi = False\n",
    "edit_lyso = False\n",
    "edit_mito = False\n",
    "edit_perox = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#x1F3C3; **Run code; no user input required** \n",
    "### &#x1F440; **See code block output for instructions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if edit_cell is True:\n",
    "    viewer.layers.clear()\n",
    "    viewer.add_image(raw_img_data)\n",
    "    viewer.add_labels(mask_seg[1])\n",
    "    print(\"Head to the Napari window!\")\n",
    "    print(\"You can edit your segmentation as needed there.\")\n",
    "    print(\"Be sure to save the new segmentation using File > Save in the Napari window. You should save it to the folder you listed as 'location_tosave_edited_segmentations'\")\n",
    "elif edit_cell is False:\n",
    "    print(\"Continue - run the next block of code\")\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if edit_nuc is True:\n",
    "    viewer.layers.clear()\n",
    "    viewer.add_image(raw_img_data)\n",
    "    viewer.add_labels(mask_seg[2])\n",
    "    print(\"Head to the Napari window!\")\n",
    "    print(\"You can edit your segmentation as needed there.\")\n",
    "    print(\"Be sure to save the new segmentation using File > Save in the Napari window. You should save it to the folder you listed as 'location_tosave_edited_segmentations'\")\n",
    "elif edit_nuc is False:\n",
    "    print(\"Continue - run the next block of code\")\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if edit_LD is True:\n",
    "    viewer.layers.clear()\n",
    "    viewer.add_image(raw_img_data)\n",
    "    viewer.add_labels(LD_seg)\n",
    "    print(\"Head to the Napari window!\")\n",
    "    print(\"You can edit your segmentation as needed there.\")\n",
    "    print(\"Be sure to save the new segmentation using File > Save in the Napari window. You should save it to the folder you listed as 'location_tosave_edited_segmentations'\")\n",
    "elif edit_LD is False:\n",
    "    print(\"Continue - run the next block of code\")\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if edit_ER is True:\n",
    "    viewer.layers.clear()\n",
    "    viewer.add_image(raw_img_data)\n",
    "    viewer.add_labels(ER_seg)\n",
    "    print(\"Head to the Napari window!\")\n",
    "    print(\"You can edit your segmentation as needed there.\")\n",
    "    print(\"Be sure to save the new segmentation using File > Save in the Napari window. You should save it to the folder you listed as 'location_tosave_edited_segmentations'\")\n",
    "elif edit_ER is False:\n",
    "    print(\"Continue - run the next block of code\")\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if edit_golgi is True:\n",
    "    viewer.layers.clear()\n",
    "    viewer.add_image(raw_img_data)\n",
    "    viewer.add_labels(golgi_seg)\n",
    "    print(\"Head to the Napari window!\")\n",
    "    print(\"You can edit your segmentation as needed there.\")\n",
    "    print(\"Be sure to save the new segmentation using File > Save in the Napari window. You should save it to the folder you listed as 'location_tosave_edited_segmentations'\")\n",
    "elif edit_golgi is False:\n",
    "    print(\"Continue - run the next block of code\")\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if edit_lyso is True:\n",
    "    viewer.layers.clear()\n",
    "    viewer.add_image(raw_img_data)\n",
    "    viewer.add_labels(lyso_seg)\n",
    "    print(\"Head to the Napari window!\")\n",
    "    print(\"You can edit your segmentation as needed there.\")\n",
    "    print(\"Be sure to save the new segmentation using File > Save in the Napari window. You should save it to the folder you listed as 'location_tosave_edited_segmentations'\")\n",
    "elif edit_lyso is False:\n",
    "    print(\"Continue - run the next block of code\")\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if edit_mito is True:\n",
    "    viewer.layers.clear()\n",
    "    viewer.add_image(raw_img_data)\n",
    "    viewer.add_labels(mito_seg)\n",
    "    print(\"Head to the Napari window!\")\n",
    "    print(\"You can edit your segmentation as needed there.\")\n",
    "    print(\"Be sure to save the new segmentation using File > Save in the Napari window. You should save it to the folder you listed as 'location_tosave_edited_segmentations'\")\n",
    "elif edit_mito is False:\n",
    "    print(\"Continue - run the next block of code\")\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if edit_perox is True:\n",
    "    viewer.layers.clear()\n",
    "    viewer.add_image(raw_img_data)\n",
    "    viewer.add_labels(perox_seg)\n",
    "    print(\"Head to the Napari window!\")\n",
    "    print(\"You can edit your segmentation as needed there.\")\n",
    "    print(\"Be sure to save the new segmentation using File > Save in the Napari window. You should save it to the folder you listed as 'location_tosave_edited_segmentations'\")\n",
    "elif edit_perox is False:\n",
    "    print(\"Continue - run the next block of code\")\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________\n",
    "# ***SAVE ALL CORRECT SEGMENTATIONS** - into one folder for quantification*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`3` - Save organelle and region segmentations into specified folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if edit_cell is True:\n",
    "    cell_seg = _import_inferred_organelle(\"cell\", raw_meta_dict, location_tosave_edited_segmentations, seg_file_type)\n",
    "    out_file_n = export_inferred_organelle(cell_seg, \"cell\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "elif edit_cell is False:\n",
    "    cell_seg = mask_seg[1]\n",
    "    out_file_n = export_inferred_organelle(cell_seg, \"cell\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if edit_nuc is True:\n",
    "    nuc_seg = _import_inferred_organelle(\"nuc\", raw_meta_dict, location_tosave_edited_segmentations, seg_file_type)\n",
    "    out_file_n = export_inferred_organelle(cell_seg, \"nuc\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "elif edit_cell is False:\n",
    "    nuc_seg = mask_seg[2]\n",
    "    out_file_n = export_inferred_organelle(cell_seg, \"nuc\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if edit_LD is True:\n",
    "    LD_seg = _import_inferred_organelle(\"LD\", raw_meta_dict, location_tosave_edited_segmentations, seg_file_type)\n",
    "    out_file_n = export_inferred_organelle(LD_seg, \"LD\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "elif edit_LD is False:\n",
    "    out_file_n = export_inferred_organelle(LD_seg, \"LD\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if edit_ER is True:\n",
    "    ER_seg = _import_inferred_organelle(\"ER\", raw_meta_dict, location_tosave_edited_segmentations, seg_file_type)\n",
    "    out_file_n = export_inferred_organelle(ER_seg, \"ER\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "elif edit_ER is False:\n",
    "    out_file_n = export_inferred_organelle(ER_seg, \"ER\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if edit_golgi is True:\n",
    "    golgi_seg = _import_inferred_organelle(\"golgi\", raw_meta_dict, location_tosave_edited_segmentations, seg_file_type)\n",
    "    out_file_n = export_inferred_organelle(golgi_seg, \"golgi\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "elif edit_golgi is False:\n",
    "    out_file_n = export_inferred_organelle(golgi_seg, \"golgi\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if edit_lyso is True:\n",
    "    lyso_seg = _import_inferred_organelle(\"lyso\", raw_meta_dict, location_tosave_edited_segmentations, seg_file_type)\n",
    "    out_file_n = export_inferred_organelle(lyso_seg, \"lyso\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "elif edit_lyso is False:\n",
    "    out_file_n = export_inferred_organelle(lyso_seg, \"lyso\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if edit_mito is True:\n",
    "    mito_seg = _import_inferred_organelle(\"mito\", raw_meta_dict, location_tosave_edited_segmentations, seg_file_type)\n",
    "    out_file_n = export_inferred_organelle(mito_seg, \"mito\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "elif edit_mito is False:\n",
    "    out_file_n = export_inferred_organelle(mito_seg, \"mito\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if edit_perox is True:\n",
    "    perox_seg = _import_inferred_organelle(\"perox\", raw_meta_dict, location_tosave_edited_segmentations, seg_file_type)\n",
    "    out_file_n = export_inferred_organelle(perox_seg, \"perox\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "elif edit_perox is False:\n",
    "    out_file_n = export_inferred_organelle(perox_seg, \"perox\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***EXECUTE QUANTIFICATION***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_for_existing_combo(contact, contact_list, splitter):\n",
    "#     for ctc in contact_list:\n",
    "#         if sorted(contact) == sorted(ctc.split(splitter)):\n",
    "#             return(ctc.split(splitter))\n",
    "#     return contact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define prototype `_batch_process_quantification` function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for convex hull errors\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", RuntimeWarning)\n",
    "\n",
    "def _batch_process_quantification(out_file_name: str,\n",
    "                                  seg_path: Union[Path,str],\n",
    "                                  out_path: Union[Path, str], \n",
    "                                  raw_path: Union[Path,str], \n",
    "                                  raw_file_type: str,\n",
    "                                  organelle_names: List[str],\n",
    "                                  organelle_channels: List[int],\n",
    "                                  region_names: List[str],\n",
    "                                  masks_file_name: list[str],\n",
    "                                  mask: str,\n",
    "                                  dist_centering_obj:str, \n",
    "                                  dist_num_bins: int,\n",
    "                                  dist_center_on: bool=False,\n",
    "                                  dist_keep_center_as_bin: bool=True,\n",
    "                                  dist_zernike_degrees: Union[int, None]=None,\n",
    "                                  include_contact_dist: bool = True,\n",
    "                                  scale:bool=True,\n",
    "                                  seg_suffix:Union[str, None]=None,\n",
    "                                  splitter: str = '_') -> int :\n",
    "    \"\"\"  \n",
    "    batch process segmentation quantification (morphology, distribution, contacts); this function is currently optimized to process images from one file folder per image type (e.g., raw, segmentation)\n",
    "    the output csv files are saved to the indicated out_path folder\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    out_file_name: str\n",
    "        the prefix to use when naming the output datatables\n",
    "    seg_path: Union[Path,str]\n",
    "        Path or str to the folder that contains the segmentation tiff files\n",
    "    out_path: Union[Path, str]\n",
    "        Path or str to the folder that the output datatables will be saved to\n",
    "    raw_path: Union[Path,str]\n",
    "        Path or str to the folder that contains the raw image files\n",
    "    raw_file_type: str\n",
    "        the file type of the raw data; ex - \".tiff\", \".czi\"\n",
    "    organelle_names: List[str]\n",
    "        a list of all organelle names that will be analyzed; the names should be the same as the suffix used to name each of the tiff segmentation files\n",
    "        Note: the intensity measurements collect per region (from get_region_morphology_3D function) will only be from channels associated to these organelles \n",
    "    organelle_channels: List[int]\n",
    "        a list of channel indices associated to respective organelle staining in the raw image; the indices should listed in same order in which the respective segmentation name is listed in organelle_names\n",
    "    region_names: List[str]\n",
    "        a list of regions, or masks, to measure; the order should correlate to the order of the channels in the \"masks\" output segmentation file\n",
    "    masks_file_name: str\n",
    "        the suffix of the \"masks\" segmentation file; ex- \"masks_B\", \"masks\", etc.\n",
    "        this function currently does not accept indivial region segmentations \n",
    "    mask: str\n",
    "        the name of the region to use as the mask when measuring the organelles; this should be one of the names listed in regions list; usually this will be the \"cell\" mask\n",
    "    dist_centering_obj:str\n",
    "        the name of the region or object to use as the centering object in the get_XY_distribution function\n",
    "    dist_num_bins: int\n",
    "        the number of bins for the get_XY_distribution function\n",
    "    dist_center_on: bool=False,\n",
    "        for get_XY_distribution:\n",
    "        True = distribute the bins from the center of the centering object\n",
    "        False = distribute the bins from the edge of the centering object\n",
    "    dist_keep_center_as_bin: bool=True\n",
    "        for get_XY_distribution:\n",
    "        True = include the centering object area when creating the bins\n",
    "        False = do not include the centering object area when creating the bins\n",
    "    dist_zernike_degrees: Union[int, None]=None\n",
    "        for get_XY_distribution:\n",
    "        the number of zernike degrees to include for the zernike shape descriptors; if None, the zernike measurements will not \n",
    "        be included in the output\n",
    "    include_contact_dist:bool=True\n",
    "        whether to include the distribution of contact sites in get_contact_metrics_3d(); True = include contact distribution\n",
    "    scale:bool=True\n",
    "        a tuple that contains the real world dimensions for each dimension in the image (Z, Y, X)\n",
    "    seg_suffix:Union[str, None]=None\n",
    "        any additional text that is included in the segmentation tiff files between the file stem and the segmentation suffix\n",
    "        TODO: this can't be None!!! need to update!!!\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    count: int\n",
    "        the number of images processed\n",
    "        \n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    count = 0\n",
    "\n",
    "    if isinstance(raw_path, str): raw_path = Path(raw_path)\n",
    "    if isinstance(seg_path, str): seg_path = Path(seg_path)\n",
    "    if isinstance(out_path, str): out_path = Path(out_path)\n",
    "    \n",
    "    if not Path.exists(out_path):\n",
    "        Path.mkdir(out_path)\n",
    "        print(f\"making {out_path}\")\n",
    "    \n",
    "    # reading list of files from the raw path\n",
    "    img_file_list = list_image_files(raw_path, raw_file_type)\n",
    "\n",
    "    # list of segmentation files to collect\n",
    "    segs_to_collect = organelle_names + masks_file_name\n",
    "\n",
    "    # containers to collect data tabels\n",
    "    org_tabs = []\n",
    "    contact_tabs = []\n",
    "    dist_tabs = []\n",
    "    region_tabs = []\n",
    "    for img_f in img_file_list:\n",
    "        count = count + 1\n",
    "        filez = find_segmentation_tiff_files(img_f, segs_to_collect, seg_path, seg_suffix)\n",
    "\n",
    "        # read in raw file and metadata\n",
    "        img_data, meta_dict = read_czi_image(filez[\"raw\"])\n",
    "\n",
    "        # create intensities from raw file as list based on the channel order provided\n",
    "        intensities = [img_data[ch] for ch in organelle_channels]\n",
    "\n",
    "        # define the scale\n",
    "        if scale is True:\n",
    "            scale_tup = meta_dict['scale']\n",
    "        else:\n",
    "            scale_tup = None\n",
    "\n",
    "        # load regions as a list based on order in list (should match order in \"masks\" file)\n",
    "        # masks = read_tiff_image(filez[masks_file_name]) \n",
    "        # regions = [masks[r] for r, region in enumerate(region_names)]\n",
    "        regions= [read_tiff_image(filez[masks_file_name[0]]), read_tiff_image(filez[masks_file_name[1]])]\n",
    "\n",
    "        # store organelle images as list\n",
    "        organelles = [read_tiff_image(filez[org]) for org in organelle_names]\n",
    "\n",
    "        org_metrics, contact_metrics, dist_metrics, region_metrics = make_all_metrics_tables(source_file=img_f,\n",
    "                                                                                             list_obj_names=organelle_names,\n",
    "                                                                                             list_obj_segs=organelles,\n",
    "                                                                                             list_intensity_img=intensities, \n",
    "                                                                                             list_region_names=region_names,\n",
    "                                                                                             list_region_segs=regions, \n",
    "                                                                                             mask=mask,\n",
    "                                                                                             dist_centering_obj=dist_centering_obj,\n",
    "                                                                                             dist_num_bins=dist_num_bins,\n",
    "                                                                                             dist_center_on=dist_center_on,\n",
    "                                                                                             dist_keep_center_as_bin=dist_keep_center_as_bin,\n",
    "                                                                                             dist_zernike_degrees=dist_zernike_degrees,\n",
    "                                                                                             scale=scale_tup,\n",
    "                                                                                             include_contact_dist=include_contact_dist,\n",
    "                                                                                             splitter=splitter)\n",
    "\n",
    "        org_tabs.append(org_metrics)\n",
    "        contact_tabs.append(contact_metrics)\n",
    "        dist_tabs.append(dist_metrics)\n",
    "        region_tabs.append(region_metrics)\n",
    "        end2 = time.time()\n",
    "        print(f\"Completed processing for {count} images in {(end2-start)/60} mins.\")\n",
    "\n",
    "    final_org = pd.concat(org_tabs, ignore_index=True)\n",
    "    final_contact = pd.concat(contact_tabs, ignore_index=True)\n",
    "    final_dist = pd.concat(dist_tabs, ignore_index=True)\n",
    "    final_region = pd.concat(region_tabs, ignore_index=True)\n",
    "\n",
    "    org_csv_path = out_path / f\"{out_file_name}organelles.csv\"\n",
    "    final_org.to_csv(org_csv_path)\n",
    "\n",
    "    contact_csv_path = out_path / f\"{out_file_name}contacts.csv\"\n",
    "    final_contact.to_csv(contact_csv_path)\n",
    "\n",
    "    dist_csv_path = out_path / f\"{out_file_name}distributions.csv\"\n",
    "    final_dist.to_csv(dist_csv_path)\n",
    "\n",
    "    region_csv_path = out_path / f\"{out_file_name}regions.csv\"\n",
    "    final_region.to_csv(region_csv_path)\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"Quantification for {count} files is COMPLETE! Files saved to '{out_path}'.\")\n",
    "    print(f\"It took {(end - start)/60} minutes to quantify these files.\")\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Run prototype `_batch_process_quantification` function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg=_batch_process_quantification(out_file_name= \"20231117_prelim\",\n",
    "                                  seg_path=\"C:/Users/zscoman/Documents/Python Scripts/Infer-subc-2D/out\",\n",
    "                                  out_path=\"C:/Users/zscoman/Documents/Python Scripts/Infer-subc-2D/data/test\", \n",
    "                                  raw_path=\"C:/Users/zscoman/Documents/Python Scripts/Infer-subc-2D/raw/shannon\",\n",
    "                                  raw_file_type = \".tiff\",\n",
    "                                  organelle_names = ['LD', 'ER', 'golgi', 'lyso', 'mito', 'perox'],\n",
    "                                  organelle_channels= [0,1,2,3,4,5],\n",
    "                                  region_names= ['nuc', 'cell'],\n",
    "                                  masks_file_name= ['nuc', 'cell'],\n",
    "                                  mask= 'cell',\n",
    "                                  dist_centering_obj='nuc', \n",
    "                                  dist_num_bins=5,\n",
    "                                  dist_center_on=False,\n",
    "                                  dist_keep_center_as_bin=True,\n",
    "                                  dist_zernike_degrees=None,\n",
    "                                  include_contact_dist= True,\n",
    "                                  scale=True,\n",
    "                                  seg_suffix=\"-\",\n",
    "                                  splitter='X')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define prototype `_batch_summary_stats` function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _batch_summary_stats(csv_path_list: List[str],\n",
    "                         out_path: str,\n",
    "                         out_preffix: str,\n",
    "                         splitter: str='X'):\n",
    "    \"\"\"\" \n",
    "    csv_path_list: List[str],\n",
    "        A list of path strings where .csv files to analyze are located.\n",
    "    out_path: str,\n",
    "        A path string where the summary data file will be output to\n",
    "    out_preffix: str\n",
    "        The prefix used to name the output file.    \n",
    "    \"\"\"\n",
    "    ds_count = 0\n",
    "    fl_count = 0\n",
    "    ###################\n",
    "    # Read in the csv files and combine them into one of each type\n",
    "    ###################\n",
    "    org_tabs = []\n",
    "    contact_tabs = []\n",
    "    dist_tabs = []\n",
    "    region_tabs = []\n",
    "\n",
    "    for loc in csv_path_list:\n",
    "        print(loc)\n",
    "        ds_count = ds_count + 1\n",
    "        loc=Path(loc)\n",
    "        files_store = sorted(loc.glob(\"*.csv\"))\n",
    "        for file in files_store:\n",
    "            fl_count = fl_count + 1\n",
    "            stem = file.stem\n",
    "\n",
    "            org = \"organelles\"\n",
    "            contacts = \"contacts\"\n",
    "            dist = \"distributions\"\n",
    "            regions = \"regions\"\n",
    "\n",
    "            if org in stem:\n",
    "                test_orgs = pd.read_csv(file, index_col=0)\n",
    "                test_orgs.insert(0, \"dataset\", stem[:-11])\n",
    "                org_tabs.append(test_orgs)\n",
    "            if contacts in stem:\n",
    "                test_contact = pd.read_csv(file, index_col=0)\n",
    "                test_contact.insert(0, \"dataset\", stem[:-9])\n",
    "                contact_tabs.append(test_contact)\n",
    "            if dist in stem:\n",
    "                test_dist = pd.read_csv(file, index_col=0)\n",
    "                test_dist.insert(0, \"dataset\", stem[:-14])\n",
    "                dist_tabs.append(test_dist)\n",
    "            if regions in stem:\n",
    "                test_regions = pd.read_csv(file, index_col=0)\n",
    "                test_regions.insert(0, \"dataset\", stem[:-8])\n",
    "                region_tabs.append(test_regions)\n",
    "            \n",
    "    org_df = pd.concat(org_tabs,axis=0, join='outer')\n",
    "    contacts_df = pd.concat(contact_tabs,axis=0, join='outer')\n",
    "    dist_df = pd.concat(dist_tabs,axis=0, join='outer')\n",
    "    regions_df = pd.concat(region_tabs,axis=0, join='outer')\n",
    "    ##########################\n",
    "    # List organelles in cell\n",
    "    ###########################\n",
    "    all_orgs = list(set(org_df.loc[:, 'object'].tolist()))\n",
    "\n",
    "    ###################\n",
    "    # adding new metrics to the original sheets\n",
    "    ###################\n",
    "    # TODO: include these labels when creating the original sheets\n",
    "    contact_cnt = contacts_df[[\"dataset\", \"image_name\", \"object\", \"label\", \"volume\"]]\n",
    "    ctc = contact_cnt[\"object\"].values.tolist()\n",
    "    ##############################################################################\n",
    "    #  Creating New methods of storing A & B\n",
    "    ###############################################################################\n",
    "    # len(max(contact_cnt[\"object\"].str.split('X'), key=len))) provides max number of organelles involved in contact\n",
    "    contact_cnt[[f\"org{cha}\" for cha in string.ascii_uppercase[:(len(max(contact_cnt[\"object\"].str.split(splitter), key=len)))]]] = contact_cnt[\"object\"].str.split(splitter, expand=True)\n",
    "    contact_cnt[[f\"{cha}_ID\" for cha in string.ascii_uppercase[:(len(max(contact_cnt[\"label\"].str.split('_'), key=len)))]]] = contact_cnt[\"label\"].str.split('_', expand=True)\n",
    "    #iterating from a to val\n",
    "    unstacked_cont = []\n",
    "    for cha in string.ascii_uppercase[:len(max(contact_cnt[\"object\"].str.split(splitter), key=len))]:\n",
    "        valid = (contact_cnt[f\"org{cha}\"] != None) & (contact_cnt[f\"{cha}_ID\"] != None)\n",
    "        contact_cnt[f\"{cha}\"] = None\n",
    "        contact_cnt.loc[valid, f\"{cha}\"] = contact_cnt[f\"org{cha}\"] + \"_\" + contact_cnt[f\"{cha}_ID\"]\n",
    "        contact_cnt_percell = contact_cnt[[\"dataset\", \"image_name\", f\"org{cha}\", f\"{cha}_ID\", \"object\", \"volume\"]].groupby([\"dataset\", \"image_name\", f\"org{cha}\", f\"{cha}_ID\", \"object\"]).agg([\"count\", \"sum\"])\n",
    "        contact_cnt_percell.columns = [\"_\".join(col_name).rstrip('_') for col_name in contact_cnt_percell.columns.to_flat_index()]\n",
    "        unstacked = contact_cnt_percell.unstack(level='object')\n",
    "        unstacked.columns = [\"_\".join(col_name).rstrip('_') for col_name in unstacked.columns.to_flat_index()]\n",
    "        unstacked = unstacked.reset_index()\n",
    "        for col in unstacked.columns:\n",
    "            if col.startswith(\"volume_count_\"):\n",
    "                newname = col.split(\"_\")[-1] + \"_count\"\n",
    "                unstacked.rename(columns={col:newname}, inplace=True)\n",
    "            if col.startswith(\"volume_sum_\"):\n",
    "                newname = col.split(\"_\")[-1] + \"_volume\"\n",
    "                unstacked.rename(columns={col:newname}, inplace=True)\n",
    "        unstacked.rename(columns={f\"org{cha}\":\"object\", f\"{cha}_ID\":\"label\"}, inplace=True)\n",
    "        unstacked.set_index(['dataset', 'image_name', 'object', 'label'])    \n",
    "        unstacked_cont.append(unstacked)\n",
    "    contact_cnt = pd.concat(unstacked_cont, axis=0).sort_index(axis=0)\n",
    "    contact_cnt = contact_cnt.groupby(['dataset', 'image_name', 'object', 'label']).sum().reset_index()                 #adds together all duplicates at the index, then resets the index\n",
    "    contact_cnt['label']=contact_cnt['label'].astype(\"Int64\")  \n",
    "    org_df = pd.merge(org_df, contact_cnt, how='left', on=['dataset', 'image_name', 'object', 'label'], sort=True)\n",
    "    org_df[contact_cnt.columns] = org_df[contact_cnt.columns].fillna(0)\n",
    "\n",
    "    ###################\n",
    "    # summary stat group\n",
    "    ###################\n",
    "    group_by = ['dataset', 'image_name', 'object']\n",
    "    sharedcolumns = [\"SA_to_volume_ratio\", \"equivalent_diameter\", \"extent\", \"euler_number\", \"solidity\", \"axis_major_length\"]\n",
    "    ag_func_standard = ['mean', 'median', 'std']\n",
    "\n",
    "    ###################\n",
    "    # summarize shared measurements between org_df and contacts_df\n",
    "    ###################\n",
    "    org_cont_tabs = []\n",
    "    for tab in [org_df, contacts_df]:\n",
    "        tab1 = tab[group_by + ['volume']].groupby(group_by).agg(['count', 'sum'] + ag_func_standard)\n",
    "        tab2 = tab[group_by + ['surface_area']].groupby(group_by).agg(['sum'] + ag_func_standard)\n",
    "        tab3 = tab[group_by + sharedcolumns].groupby(group_by).agg(ag_func_standard)\n",
    "        shared_metrics = pd.merge(tab1, tab2, 'outer', on=group_by)\n",
    "        shared_metrics = pd.merge(shared_metrics, tab3, 'outer', on=group_by)\n",
    "        org_cont_tabs.append(shared_metrics)\n",
    "\n",
    "    org_summary = org_cont_tabs[0]\n",
    "    contact_summary = org_cont_tabs[1]\n",
    "\n",
    "    ###################\n",
    "    # group metrics from regions_df similar to the above\n",
    "    ###################\n",
    "    regions_summary = regions_df[group_by + ['volume', 'surface_area'] + sharedcolumns].set_index(group_by)\n",
    "\n",
    "    ###################\n",
    "    # summarize extra metrics from org_df\n",
    "    ###################\n",
    "    columns2 = [col for col in org_df.columns if col.endswith((\"_count\", \"_volume\"))]\n",
    "    contact_counts_summary = org_df[group_by + columns2].groupby(group_by).agg(['sum'] + ag_func_standard)\n",
    "    org_summary = pd.merge(org_summary, contact_counts_summary, 'outer', on=group_by)#left_on=group_by, right_on=True)\n",
    "\n",
    "    ###################\n",
    "    # summarize distribution measurements\n",
    "    ###################\n",
    "    # organelle distributions\n",
    "    hist_dfs = []\n",
    "    for ind in dist_df.index:\n",
    "        selection = dist_df.loc[[ind]]\n",
    "        bins_df = pd.DataFrame()\n",
    "        wedges_df = pd.DataFrame()\n",
    "        Z_df = pd.DataFrame()\n",
    "\n",
    "        bins_df[['bins', 'masks', 'obj']] = selection[['XY_bins', 'XY_mask_vox_cnt_perbin', 'XY_obj_vox_cnt_perbin']]\n",
    "        wedges_df[['bins', 'masks', 'obj']] = selection[['XY_wedges', 'XY_mask_vox_cnt_perwedge', 'XY_obj_vox_cnt_perwedge']]\n",
    "        Z_df[['bins', 'masks', 'obj']] = selection[['Z_slices', 'Z_mask_vox_cnt', 'Z_obj_vox_cnt']]\n",
    "\n",
    "        dfs = [selection[['dataset', 'image_name', 'object']].reset_index()]\n",
    "        for df, prefix in zip([bins_df, wedges_df, Z_df], [\"XY_bins_\", \"XY_wedges_\", \"Z_slices_\"]):\n",
    "            single_df = pd.DataFrame(list(zip(df[\"bins\"].values[0][1:-1].split(\", \"), \n",
    "                                            df[\"obj\"].values[0][1:-1].split(\", \"), \n",
    "                                            df[\"masks\"].values[0][1:-1].split(\", \"))), columns =['bins', 'obj', 'mask']).astype(int)\n",
    "\n",
    "            if \"Z_\" in prefix:\n",
    "                single_df =  single_df.drop(single_df[single_df['mask'] == 0].index)\n",
    "                single_df['bins'] = (single_df[\"bins\"]/max(single_df.bins)*10).apply(np.floor)\n",
    "        \n",
    "            single_df['mask_fract'] = single_df['mask']/single_df['mask'].max()\n",
    "            single_df['obj_norm'] = (single_df[\"obj\"]/single_df[\"mask_fract\"]).fillna(0)\n",
    "            single_df['portion_per_bin'] = (single_df[\"obj\"] / single_df[\"obj\"].sum())*100\n",
    "\n",
    "            # single_df['mask_fract'] = single_df['mask']/single_df['mask'].max()\n",
    "            # single_df['obj_norm'] = (single_df[\"obj\"]/single_df[\"mask_fract\"]).fillna(0)\n",
    "            # single_df['portion_per_bin'] = (single_df[\"obj\"] / single_df[\"obj\"].sum())*100\n",
    "\n",
    "            # if \"Z_\" in prefix:\n",
    "            #     single_df['bins'] = (single_df[\"bins\"]/max(single_df.bins)*10).apply(np.floor)\n",
    "\n",
    "\n",
    "            sumstats_df = pd.DataFrame()\n",
    "\n",
    "            s = single_df['bins'].repeat(single_df['obj_norm'])\n",
    "            sumstats_df['hist_mean']=[s.mean()]\n",
    "            sumstats_df['hist_median']=[s.median()]\n",
    "            if single_df['obj_norm'].sum() != 0: sumstats_df['hist_mode']=[s.mode()[0]]\n",
    "            else: sumstats_df['hist_mode']=['NaN']\n",
    "            sumstats_df['hist_min']=[s.min()]\n",
    "            sumstats_df['hist_max']=[s.max()]\n",
    "            sumstats_df['hist_range']=[s.max() - s.min()]\n",
    "            sumstats_df['hist_stdev']=[s.std()]\n",
    "            sumstats_df['hist_skew']=[s.skew()]\n",
    "            sumstats_df['hist_kurtosis']=[s.kurtosis()]\n",
    "            sumstats_df['hist_var']=[s.var()]\n",
    "            sumstats_df.columns = [prefix+col for col in sumstats_df.columns]\n",
    "            dfs.append(sumstats_df.reset_index())\n",
    "        combined_df = pd.concat(dfs, axis=1).drop(columns=\"index\")\n",
    "        hist_dfs.append(combined_df)\n",
    "    dist_org_summary = pd.concat(hist_dfs, ignore_index=True)\n",
    "\n",
    "    # nucleus distribution\n",
    "    nuc_dist_df = dist_df[[\"dataset\", \"image_name\", \n",
    "                        \"XY_bins\", \"XY_center_vox_cnt_perbin\", \"XY_mask_vox_cnt_perbin\",\n",
    "                        \"XY_wedges\", \"XY_center_vox_cnt_perwedge\", \"XY_mask_vox_cnt_perwedge\",\n",
    "                        \"Z_slices\", \"Z_center_vox_cnt\", \"Z_mask_vox_cnt\"]].set_index([\"dataset\", \"image_name\"])\n",
    "    nuc_hist_dfs = []\n",
    "    for idx in nuc_dist_df.index.unique():\n",
    "        selection = nuc_dist_df.loc[idx].iloc[[0]].reset_index()\n",
    "        bins_df = pd.DataFrame()\n",
    "        wedges_df = pd.DataFrame()\n",
    "        Z_df = pd.DataFrame()\n",
    "\n",
    "        bins_df[['bins', 'center', 'masks']] = selection[['XY_bins', 'XY_center_vox_cnt_perbin', 'XY_mask_vox_cnt_perbin']]\n",
    "        wedges_df[['bins', 'center', 'masks']] = selection[['XY_wedges', 'XY_center_vox_cnt_perwedge', 'XY_mask_vox_cnt_perwedge']]\n",
    "        Z_df[['bins', 'center', 'masks']] = selection[['Z_slices', 'Z_center_vox_cnt', 'Z_mask_vox_cnt']]\n",
    "\n",
    "        dfs = [selection[['dataset', 'image_name']]]\n",
    "        for df, prefix in zip([bins_df, wedges_df, Z_df], [\"XY_bins_\", \"XY_wedges_\", \"Z_slices_\"]):\n",
    "            single_df = pd.DataFrame(list(zip(df[\"bins\"].values[0][1:-1].split(\", \"), \n",
    "                                            df[\"masks\"].values[0][1:-1].split(\", \"),\n",
    "                                            df[\"center\"].values[0][1:-1].split(\", \"))), columns =['bins', 'mask', 'obj']).astype(int)\n",
    "            # single_df['mask_fract'] = single_df['mask']/single_df['mask'].max()\n",
    "            # single_df['obj_norm'] = (single_df[\"obj\"]/single_df[\"mask_fract\"]).fillna(0)\n",
    "            # single_df['portion_per_bin'] = (single_df[\"obj\"] / single_df[\"obj\"].sum())*100\n",
    "            # if \"Z_\" in prefix:\n",
    "            #     single_df['bins'] = (single_df[\"bins\"]/max(single_df.bins)*10).apply(np.floor)\n",
    "\n",
    "            if \"Z_\" in prefix:\n",
    "                single_df =  single_df.drop(single_df[single_df['mask'] == 0].index)\n",
    "                single_df['bins'] = (single_df[\"bins\"]/max(single_df.bins)*10).apply(np.floor)\n",
    "        \n",
    "            single_df['mask_fract'] = single_df['mask']/single_df['mask'].max()\n",
    "            single_df['obj_norm'] = (single_df[\"obj\"]/single_df[\"mask_fract\"]).fillna(0)\n",
    "            single_df['portion_per_bin'] = (single_df[\"obj\"] / single_df[\"obj\"].sum())*100\n",
    "\n",
    "            sumstats_df = pd.DataFrame()\n",
    "\n",
    "            s = single_df['bins'].repeat(single_df['obj_norm'])\n",
    "            sumstats_df['hist_mean']=[s.mean()]\n",
    "            sumstats_df['hist_median']=[s.median()]\n",
    "            if single_df['obj_norm'].sum() != 0: sumstats_df['hist_mode']=[s.mode()[0]]\n",
    "            else: sumstats_df['hist_mode']=['NaN']\n",
    "            sumstats_df['hist_min']=[s.min()]\n",
    "            sumstats_df['hist_max']=[s.max()]\n",
    "            sumstats_df['hist_range']=[s.max() - s.min()]\n",
    "            sumstats_df['hist_stdev']=[s.std()]\n",
    "            sumstats_df['hist_skew']=[s.skew()]\n",
    "            sumstats_df['hist_kurtosis']=[s.kurtosis()]\n",
    "            sumstats_df['hist_var']=[s.var()]\n",
    "            sumstats_df.columns = [prefix+col for col in sumstats_df.columns]\n",
    "            dfs.append(sumstats_df.reset_index())\n",
    "        combined_df = pd.concat(dfs, axis=1).drop(columns=\"index\")\n",
    "        nuc_hist_dfs.append(combined_df)\n",
    "    dist_center_summary = pd.concat(nuc_hist_dfs, ignore_index=True)\n",
    "    dist_center_summary.insert(2, column=\"object\", value=\"nuc\")\n",
    "\n",
    "    dist_summary = pd.concat([dist_org_summary, dist_center_summary], axis=0).set_index(group_by).sort_index()\n",
    "\n",
    "    ###################\n",
    "    # add normalization\n",
    "    ###################\n",
    "    # organelle area fraction\n",
    "    area_fractions = []\n",
    "    for idx in org_summary.index.unique():\n",
    "        org_vol = org_summary.loc[idx][('volume', 'sum')]\n",
    "        cell_vol = regions_summary.loc[idx[:-1] + ('cell',)][\"volume\"]\n",
    "        afrac = org_vol/cell_vol\n",
    "        area_fractions.append(afrac)\n",
    "    org_summary[('volume', 'fraction')] = area_fractions\n",
    "    # TODO: add in line to reorder the level=0 columns here\n",
    "\n",
    "    # contact sites volume normalized\n",
    "    # norm_toA_list = []\n",
    "    # norm_toB_list = []\n",
    "    norm_to_list = {}\n",
    "    for col in contact_summary.index:\n",
    "        for idx, cha in enumerate(string.ascii_uppercase[:len(max(contact_summary.index.get_level_values('object').str.split(splitter), key=len))]):\n",
    "            if cha not in norm_to_list:\n",
    "                norm_to_list[f\"{cha}\"] = []\n",
    "            if ((idx+1) <= len(col[-1].split(splitter))):\n",
    "                norm_to_list[f\"{cha}\"].append(contact_summary.loc[col][('volume', 'sum')]/org_summary.loc[col[:-1]+(col[-1].split(splitter)[idx],)][('volume', 'sum')])\n",
    "            else:\n",
    "                norm_to_list[f\"{cha}\"].append(None)\n",
    "    for cha in string.ascii_uppercase[:len(max(contact_summary.index.get_level_values('object').str.split(splitter), key=len))]:\n",
    "        contact_summary[('volume', f'norm_to_{cha}')] = norm_to_list[f\"{cha}\"]\n",
    "        # norm_toA_list.append(contact_summary.loc[col][('volume', 'sum')]/org_summary.loc[col[:-1]+(col[-1].split(splitter)[0],)][('volume', 'sum')])\n",
    "        # norm_toB_list.append(contact_summary.loc[col][('volume', 'sum')]/org_summary.loc[col[:-1]+(col[-1].split(splitter)[1],)][('volume', 'sum')])\n",
    "    # contact_summary[('volume', 'norm_to_A')] = norm_toA_list\n",
    "    # contact_summary[('volume', 'norm_to_B')] = norm_toB_list\n",
    "\n",
    "    # number and area of individuals organelle involved in contact\n",
    "    cont_cnt = org_df[group_by]\n",
    "    cont_cnt[[col.split('_')[0] for col in org_df.columns if col.endswith((\"_count\"))]] = org_df[[col for col in org_df.columns if col.endswith((\"_count\"))]].astype(bool)\n",
    "    cont_cnt_perorg = cont_cnt.groupby(group_by).agg('sum')\n",
    "    cont_cnt_perorg.columns = pd.MultiIndex.from_product([cont_cnt_perorg.columns, ['count_in']])\n",
    "    for col in cont_cnt_perorg.columns:\n",
    "        cont_cnt_perorg[(col[0], 'num_fraction_in')] = cont_cnt_perorg[col].values/org_summary[('volume', 'count')].values\n",
    "    cont_cnt_perorg.sort_index(axis=1, inplace=True)\n",
    "    org_summary = pd.merge(org_summary, cont_cnt_perorg, on=group_by, how='outer')\n",
    "\n",
    "\n",
    "    ###################\n",
    "    # flatten datasheets and combine\n",
    "    # TODO: restructure this so that all of the datasheets and unstacked and then reorded based on shared level 0 columns before flattening\n",
    "    ###################\n",
    "    # org flattening\n",
    "    org_final = org_summary.unstack(-1)\n",
    "    for col in org_final.columns:\n",
    "        if col[1] in ('count_in', 'num_fraction_in') or col[0].endswith(('_count', '_volume')):\n",
    "            if col[2] not in col[0]:\n",
    "                org_final.drop(col,axis=1, inplace=True)\n",
    "    ########################################################################\n",
    "    # MAKING new_col_order flexible to work with any organelle input values and combo number\n",
    "    #######################################################################\n",
    "    new_col_order = ['dataset', 'image_name', 'object', 'volume', 'surface_area', 'SA_to_volume_ratio', \n",
    "                     'equivalent_diameter', 'extent', 'euler_number', 'solidity', 'axis_major_length'] \n",
    "    all_combos = []\n",
    "    for n in list(map(lambda x:x+2, (range(len(all_orgs)-1)))):\n",
    "            for o in itertools.combinations(all_orgs, n):\n",
    "                all_combos.append(check_for_existing_combo(o, ctc, splitter))\n",
    "    combos = [splitter.join(cont) for cont in all_combos]\n",
    "    for combo in combos:\n",
    "        new_col_order += [f\"{combo}\", f\"{combo}_count\", f\"{combo}_volume\"]\n",
    "    new_cols = org_final.columns.reindex(new_col_order, level=0)\n",
    "    org_final = org_final.reindex(columns=new_cols[0])\n",
    "    org_final.columns = [\"_\".join((col_name[-1], col_name[1], col_name[0])) for col_name in org_final.columns.to_flat_index()]\n",
    "\n",
    "    #renaming, filling \"NaN\" with 0 when needed, and removing ER_std columns\n",
    "    for col in org_final.columns:\n",
    "        if '_count_in_' or '_fraction_in_' in col:\n",
    "            org_final[col] = org_final[col].fillna(0)\n",
    "        if col.endswith((\"_count_volume\",\"_sum_volume\", \"_mean_volume\", \"_median_volume\")):\n",
    "            org_final[col] = org_final[col].fillna(0)\n",
    "        if col.endswith(\"_count_volume\"):\n",
    "            org_final.rename(columns={col:col.split(\"_\")[0]+\"_count\"}, inplace=True)\n",
    "        if col.startswith(\"ER_std_\"):\n",
    "            org_final.drop(columns=[col], inplace=True)\n",
    "    org_final = org_final.reset_index()\n",
    "\n",
    "    # contacts flattened\n",
    "    contact_final = contact_summary.unstack(-1)\n",
    "    contact_final.columns = [\"_\".join((col_name[-1], col_name[1], col_name[0])) for col_name in contact_final.columns.to_flat_index()]\n",
    "\n",
    "    #renaming and filling \"NaN\" with 0 when needed\n",
    "    for col in contact_final.columns:\n",
    "        if col.endswith((\"_count_volume\",\"_sum_volume\", \"_mean_volume\", \"_median_volume\")):\n",
    "            contact_final[col] = contact_final[col].fillna(0)\n",
    "        if col.endswith(\"_count_volume\"):\n",
    "            contact_final.rename(columns={col:col.split(\"_\")[0]+\"_count\"}, inplace=True)\n",
    "    contact_final = contact_final.reset_index()\n",
    "\n",
    "    # distributions flattened\n",
    "    dist_final = dist_summary.unstack(-1)\n",
    "    dist_final.columns = [\"_\".join((col_name[1], col_name[0])) for col_name in dist_final.columns.to_flat_index()]\n",
    "    dist_final = dist_final.reset_index()\n",
    "\n",
    "    # regions flattened & normalization added\n",
    "    regions_final = regions_summary.unstack(-1)\n",
    "    regions_final.columns = [\"_\".join((col_name[1], col_name[0])) for col_name in regions_final.columns.to_flat_index()]\n",
    "    regions_final['nuc_area_fraction'] = regions_final['nuc_volume'] / regions_final['cell_volume']\n",
    "    regions_final = regions_final.reset_index()\n",
    "\n",
    "    # combining them all\n",
    "    combined = pd.merge(org_final, contact_final, on=[\"dataset\", \"image_name\"], how=\"outer\")\n",
    "    combined = pd.merge(combined, dist_final, on=[\"dataset\", \"image_name\"], how=\"outer\")\n",
    "    combined = pd.merge(combined, regions_final, on=[\"dataset\", \"image_name\"], how=\"outer\").set_index([\"dataset\", \"image_name\"])\n",
    "    combined.columns = [col.replace('sum', 'total') for col in combined.columns]\n",
    "\n",
    "    ###################\n",
    "    # export summary sheets\n",
    "    ###################\n",
    "    org_summary.to_csv(out_path + f\"/{out_preffix}per_org_summarystats.csv\")\n",
    "    contact_summary.to_csv(out_path + f\"/{out_preffix}per_contact_summarystats.csv\")\n",
    "    dist_summary.to_csv(out_path + f\"/{out_preffix}distribution_summarystats.csv\")\n",
    "    regions_summary.to_csv(out_path + f\"/{out_preffix}per_region_summarystats.csv\")\n",
    "    combined.to_csv(out_path + f\"/{out_preffix}summarystats_combined.csv\")\n",
    "\n",
    "    print(f\"Processing of {fl_count} files from {ds_count} dataset(s) is complete.\")\n",
    "    return f\"{fl_count} files from {ds_count} dataset(s) were processed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Run prototype `_batch_summary_stats` function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=_batch_summary_stats(csv_path_list=[\"C:/Users/zscoman/Documents/Python Scripts/Infer-subc-2D/data/test\"],\n",
    "                         out_path=\"C:/Users/zscoman/Documents/Python Scripts/Infer-subc-2D/data/test/sumstat\",\n",
    "                         out_preffix=\"20231117_prelim_\",\n",
    "                         splitter=\"X\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infer-subc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting **Summary Statistics** across multiple experiments - Part 2.6\n",
    "--------------------\n",
    "Now that all a function has been built to quantify features of **organelle** and **regions** across multiple cells, we can create a function that **summarizes** the resulting metrics.\n",
    "\n",
    "## **OBJECTIVE**\n",
    "### <input type=\"checkbox\"/> Summarize ***organelle and region quantification***\n",
    "In this notebook, the logic for restructing and summarizing the **organelle and region** metric tables from the combined quantification is outlined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "## **Batch Summary Stats**\n",
    "\n",
    "### summary of steps\n",
    "\n",
    "ðŸ› ï¸ **BUILD FUNCTION PROTOTYPE**\n",
    "\n",
    "- **`0`** - Establish csv paths *(preliminary step)*\n",
    "\n",
    "- **`1`** - Read in and categorize csv files\n",
    "\n",
    "    - read in and categorize csv files for all listed paths\n",
    "    - Combine comprehensive metrics tables to be summarized and restructured\n",
    "\n",
    "- **`2`** - Restructure comprehensive organelle two-way interaction metrics table\n",
    "\n",
    "    - breakdown the interaction table column names\n",
    "    - group observations by the **first** organelle involved in interactions\n",
    "    - unstack the grouped table to create a column for every unique organelle interaction type\n",
    "    - correct column names for unstacked tables to accurately describe the **first** organelle involved in each unique interaction site\n",
    "    - repeat last three substeps for the **second** organelle involved in the interactions\n",
    "    - combine and merge the data from **both** unstacked tables to include interaction metrics from all organelle objects\n",
    "\n",
    "- **`3`** - Apply aggregate statistics for summarization\n",
    "\n",
    "    - determine aggregate statistics to be applied per organelle object\n",
    "    - summarize metrics between the organelle morphology and interaction tables\n",
    "    - summarize metrics in the region morphology table\n",
    "    - summarize additional metrics in the organelle morphology table\n",
    "\n",
    "- **`4`** - Restructure distribution metrics tables\n",
    "\n",
    "    - for XY-distribution collect summary statistics for voxel bins and wedges\n",
    "    - for Z-distribution collect summary statistics for voxel bins and wedges\n",
    "    - calculate the coefficient of variation for the **mean**, **median**, **standard deviation** for the XY-distribution bin values\n",
    "    - repeat the first two substeps for the nucleus distribution metrics\n",
    "    - combine nucleus and organelle distribution tables\n",
    "\n",
    "\n",
    "- **`5`** - Add normalized metrics\n",
    "\n",
    "    - calculate fraction of cell area taken up by the organelles\n",
    "    - calculate fraction of organelle objects involved in specific interorganelle contacts\n",
    "\n",
    "- **`6`** - Unstack and finalize summary stats tables\n",
    "\n",
    "    - unstack and reorder organelle morphology summary table columns\n",
    "    - fill \"NaN\" values with 0 when necessary to final organelle morphology summary table\n",
    "    - unstack and reorder organelle interactions summary table columns\n",
    "    - fill \"NaN\" values with 0 when necessary to final organelle interactions summary table\n",
    "    - unstack and reorder distribution measurements summary table columns to create finalized table\n",
    "    - unstack and reorder region morphology summary table columns\n",
    "    - add normalization to finalize region morphology summary table\n",
    "    - combine all four tables to create a complete summary table\n",
    "\n",
    "- **`7`** - Export summary stats tables as .csv files\n",
    "\n",
    "âš™ï¸ **EXECUTE FUNCTION PROTOTYPE**\n",
    "\n",
    "- Define prototype `_batch_summary_stats` function\n",
    "\n",
    "- Run prototype `_batch_summary_stats` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **IMPORTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &#x1F3C3; **Run code; no user input required**\n",
    "\n",
    "&#x1F453; **FYI:** This code block loads all of the necessary python packages and functions you will need for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from infer_subc.utils.stats import *\n",
    "from infer_subc.utils.stats_helpers import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***BUILD FUNCTION PROTOTYPE***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`0` - Establish csv paths *(preliminary step)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &#x1F6D1; &#x270D; **User Input Required:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of file paths to be included in the summary\n",
    "\n",
    "# All of the following options are correctly set to work with the sample data;\n",
    "# If you are not using the sample data, please edit the below as necessary.\n",
    "csv_path_list = [Path(os.getcwd()).parents[1] / \"sample_data\" /  \"batch_example\" / \"quant\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`1` - Read in and categorize csv files**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- read in and categorize csv files for all listed paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_count = 0\n",
    "fl_count = 0\n",
    "\n",
    "org_tabs = []\n",
    "contact_tabs = []\n",
    "dist_tabs = []\n",
    "region_tabs = []\n",
    "\n",
    "for loc in csv_path_list:\n",
    "    ds_count = ds_count + 1\n",
    "    loc=Path(loc)\n",
    "    files_store = sorted(loc.glob(\"*.csv\"))\n",
    "    for file in files_store:\n",
    "        fl_count = fl_count + 1\n",
    "        stem = file.stem\n",
    "\n",
    "        org = \"organelles\"\n",
    "        contacts = \"contacts\"\n",
    "        dist = \"distributions\"\n",
    "        regions = \"_regions\"\n",
    "\n",
    "        if org in stem:\n",
    "            test_orgs = pd.read_csv(file, index_col=0)\n",
    "            test_orgs.insert(0, \"dataset\", stem[:-11])\n",
    "            org_tabs.append(test_orgs)\n",
    "        if contacts in stem:\n",
    "            test_contact = pd.read_csv(file, index_col=0)\n",
    "            test_contact.insert(0, \"dataset\", stem[:-9])\n",
    "            contact_tabs.append(test_contact)\n",
    "        if dist in stem:\n",
    "            test_dist = pd.read_csv(file, index_col=0)\n",
    "            test_dist.insert(0, \"dataset\", stem[:-14])\n",
    "            dist_tabs.append(test_dist)\n",
    "        if regions in stem:\n",
    "            test_regions = pd.read_csv(file, index_col=0)\n",
    "            test_regions.insert(0, \"dataset\", stem[:-8])\n",
    "            region_tabs.append(test_regions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combine comprehensive metrics tables to be summarized and restructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_df = pd.concat(org_tabs,axis=0, join='outer')\n",
    "contacts_df = pd.concat(contact_tabs,axis=0, join='outer')\n",
    "dist_df = pd.concat(dist_tabs,axis=0, join='outer')\n",
    "regions_df = pd.concat(region_tabs,axis=0, join='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contacts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`2` - Restructure comprehensive two-way organelle interaction metrics table**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ###### **ðŸ“ Please note that in the following steps, a specific procedure will be repeated to ensure that all unique organelle objects are described by their interaction metrics, regardless of whether they are the first organelle (A) or the second organelle (B) involved in the two-way contact.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- breakdown the interaction table column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_cnt = contacts_df[[\"dataset\", \"image_name\", \"object\", \"label\", \"volume\"]]\n",
    "contact_cnt[[\"orgA\", \"orgB\"]] = contact_cnt[\"object\"].str.split('X', expand=True)\n",
    "contact_cnt[[\"A_ID\", \"B_ID\"]] = contact_cnt[\"label\"].str.split('_', expand=True)\n",
    "contact_cnt[\"A\"] = contact_cnt[\"orgA\"] +\"_\" + contact_cnt[\"A_ID\"].astype(str)\n",
    "contact_cnt[\"B\"] = contact_cnt[\"orgB\"] +\"_\" + contact_cnt[\"B_ID\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- group observations by the **first** organelle involved in interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_cnt_percell = contact_cnt[[\"dataset\", \"image_name\", \"orgA\", \"A_ID\", \"object\", \"volume\"]].groupby([\"dataset\", \"image_name\", \"orgA\", \"A_ID\", \"object\"]).agg([\"count\", \"sum\"])\n",
    "contact_cnt_percell.columns = [\"_\".join(col_name).rstrip('_') for col_name in contact_cnt_percell.columns.to_flat_index()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- unstack the grouped table to create a column for every unique organelle interaction type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unstacked = contact_cnt_percell.unstack(level='object')\n",
    "unstacked.columns = [\"_\".join(col_name).rstrip('_') for col_name in unstacked.columns.to_flat_index()]\n",
    "unstacked = unstacked.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- correct column names for unstacked tables to accurately describe the **first** organelle involved in each unique interaction site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixes the count and volume metrics\n",
    "for col in unstacked.columns:\n",
    "    if col.startswith(\"volume_count_\"):\n",
    "        newname = col.split(\"_\")[-1] + \"_count\"\n",
    "        unstacked.rename(columns={col:newname}, inplace=True)\n",
    "    if col.startswith(\"volume_sum_\"):\n",
    "        newname = col.split(\"_\")[-1] + \"_volume\"\n",
    "        unstacked.rename(columns={col:newname}, inplace=True)\n",
    "\n",
    "# first organelle is simply referred to as object\n",
    "# label of first organelle is simply reffered to as label\n",
    "unstacked.rename(columns={\"orgA\":\"object\", \"A_ID\":\"label\"}, inplace=True)\n",
    "unstacked.set_index(['dataset', 'image_name', 'object', 'label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- repeat last three substeps for the **second** organelle involved in the interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_percellB = contact_cnt[[\"dataset\", \"image_name\", \"orgB\", \"B_ID\", \"object\", \"volume\"]].groupby([\"dataset\", \"image_name\", \"orgB\", \"B_ID\", \"object\"]).agg([\"count\", \"sum\"])\n",
    "contact_percellB.columns = [\"_\".join(col_name).rstrip('_') for col_name in contact_percellB.columns.to_flat_index()]\n",
    "unstackedB = contact_percellB.unstack(level='object')\n",
    "unstackedB.columns = [\"_\".join(col_name).rstrip('_') for col_name in unstackedB.columns.to_flat_index()]\n",
    "unstackedB = unstackedB.reset_index()\n",
    "for col in unstackedB.columns:\n",
    "    if col.startswith(\"volume_count_\"):\n",
    "        newname = col.split(\"_\")[-1] + \"_count\"\n",
    "        unstackedB.rename(columns={col:newname}, inplace=True)\n",
    "    if col.startswith(\"volume_sum_\"):\n",
    "        newname = col.split(\"_\")[-1] + \"_volume\"\n",
    "        unstackedB.rename(columns={col:newname}, inplace=True)\n",
    "unstackedB.rename(columns={\"orgB\":\"object\", \"B_ID\":\"label\"}, inplace=True)\n",
    "unstackedB.set_index(['dataset', 'image_name', 'object', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- combine and merge the data from **both** unstacked tables to include interaction metrics from all organelle objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_cnt = pd.concat([unstacked, unstackedB], axis=0).sort_index(axis=0)\n",
    "contact_cnt = contact_cnt.groupby(['dataset', 'image_name', 'object', 'label']).sum().reset_index()\n",
    "contact_cnt['label']=contact_cnt['label'].astype(\"Int64\")\n",
    "\n",
    "org_df = pd.merge(org_df, contact_cnt, how='left', on=['dataset', 'image_name', 'object', 'label'], sort=True)\n",
    "org_df[contact_cnt.columns] = org_df[contact_cnt.columns].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_cnt_percell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unstacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`3` - Apply aggregate statistics for summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- determine aggregate statistics to be applied per organelle object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# summary stat group\n",
    "###################\n",
    "\n",
    "# ensure summary statistics are applied on a per organelle object level\n",
    "group_by = ['dataset', 'image_name', 'object']\n",
    "\n",
    "# metrics to be observed\n",
    "sharedcolumns = [\"SA_to_volume_ratio\", \"equivalent_diameter\", \"extent\", \"euler_number\", \"solidity\", \"axis_major_length\"]\n",
    "\n",
    "# statistical functions to be performed on the metrics\n",
    "ag_func_standard = ['mean', 'median', 'std']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- summarize shared metrics between the organelle morphology and interaction tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# summarize shared measurements between org_df and contacts_df\n",
    "###################\n",
    "org_cont_tabs = []\n",
    "for tab in [org_df, contacts_df]:\n",
    "    tab1 = tab[group_by + ['volume']].groupby(group_by).agg(['count', 'sum'] + ag_func_standard)\n",
    "    tab2 = tab[group_by + ['surface_area']].groupby(group_by).agg(['sum'] + ag_func_standard)\n",
    "    tab3 = tab[group_by + sharedcolumns].groupby(group_by).agg(ag_func_standard)\n",
    "    shared_metrics = pd.merge(tab1, tab2, 'outer', on=group_by)\n",
    "    shared_metrics = pd.merge(shared_metrics, tab3, 'outer', on=group_by)\n",
    "    org_cont_tabs.append(shared_metrics)\n",
    "\n",
    "org_summary = org_cont_tabs[0]\n",
    "contact_summary = org_cont_tabs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- summarize metrics in the region morphology table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# group metrics from regions_df similar to the above\n",
    "###################\n",
    "regions_summary = regions_df[group_by + ['volume', 'surface_area'] + sharedcolumns].set_index(group_by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- summarize additional metrics in the organelle morphology table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# summarize extra metrics from org_df\n",
    "###################\n",
    "columns2 = [col for col in org_df.columns if col.endswith((\"_count\", \"_volume\"))]\n",
    "contact_counts_summary = org_df[group_by + columns2].groupby(group_by).agg(['sum'] + ag_func_standard)\n",
    "org_summary = pd.merge(org_summary, contact_counts_summary, 'outer', on=group_by)#left_on=group_by, right_on=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`4` - Restructure distribution metrics tables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for XY-distribution collect summary statistics for voxel bins and wedges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for Z-distribution collect summary statistics for voxel bins and wedges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ###### Statistics collected: **mean, median, mode, minimum, maximum, range, standard deviation, skew, kurtosis and variance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- calculate the coefficient of variation for the **mean**, **median**, **standard deviation** for the XY-distribution bin values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_dfs = []\n",
    "for ind in range(0,1):#len(dist_df.index)):\n",
    "    selection = dist_df.iloc[[ind]] #    selection = dist_df.loc[[ind]]\n",
    "    bins_df = pd.DataFrame()\n",
    "    wedges_df = pd.DataFrame()\n",
    "    Z_df = pd.DataFrame()\n",
    "    CV_df = pd.DataFrame()\n",
    "\n",
    "    bins_df[['bins', 'masks', 'obj']] = selection[['XY_bins', 'XY_mask_vox_cnt_perbin', 'XY_obj_vox_cnt_perbin']]\n",
    "    wedges_df[['bins', 'masks', 'obj']] = selection[['XY_wedges', 'XY_mask_vox_cnt_perwedge', 'XY_obj_vox_cnt_perwedge']]\n",
    "    Z_df[['bins', 'masks', 'obj']] = selection[['Z_slices', 'Z_mask_vox_cnt', 'Z_obj_vox_cnt']]\n",
    "\n",
    "    dfs = [selection[['dataset', 'image_name', 'object']].reset_index()]\n",
    "    for df, prefix in zip([bins_df, wedges_df, Z_df], [\"XY_bins_\", \"XY_wedges_\", \"Z_slices_\"]):\n",
    "        single_df = pd.DataFrame(list(zip(df[\"bins\"].values[0][1:-1].split(\", \"), \n",
    "                                        df[\"obj\"].values[0][1:-1].split(\", \"), \n",
    "                                        df[\"masks\"].values[0][1:-1].split(\", \"))), columns =['bins', 'obj', 'mask']).astype(int)\n",
    "        \n",
    "        if \"Z_\" in prefix:\n",
    "            single_df =  single_df.drop(single_df[single_df['mask'] == 0].index)\n",
    "            single_df['bins'] = (single_df[\"bins\"]/max(single_df.bins)*9.99).apply(np.floor)+1\n",
    "            single_df = single_df.groupby(\"bins\").agg(['sum']).reset_index()\n",
    "            single_df.columns = ['bins',\"obj\",\"mask\"]\n",
    "    \n",
    "        single_df['mask_fract'] = single_df['mask']/single_df['mask'].max()\n",
    "        # single_df['obj_normed_tocell'] = (single_df[\"obj\"]*single_df[\"mask_fract\"]).fillna(0)\n",
    "        single_df['obj_perc_per_bin'] = (single_df[\"obj\"] / single_df[\"obj\"].sum())*100\n",
    "        single_df['obj_portion_normed_tobin'] = (single_df[\"obj_perc_per_bin\"]/single_df[\"mask_fract\"]).fillna(0)\n",
    "\n",
    "        sumstats_df = pd.DataFrame()\n",
    "\n",
    "        s = single_df['bins'].repeat(single_df['obj_portion_normed_tobin']*100)\n",
    "        ###################################\n",
    "        #SUB-STEPS 1 & 2\n",
    "        ###################################\n",
    "        sumstats_df['hist_mean']=[s.mean()]\n",
    "        sumstats_df['hist_median']=[s.median()]\n",
    "        if single_df['obj_portion_normed_tobin'].sum() != 0: sumstats_df['hist_mode']=[s.mode().iloc[0]]\n",
    "        else: sumstats_df['hist_mode']=['NaN']\n",
    "        sumstats_df['hist_min']=[s.min()]\n",
    "        sumstats_df['hist_max']=[s.max()]\n",
    "        sumstats_df['hist_range']=[s.max() - s.min()]\n",
    "        sumstats_df['hist_stdev']=[s.std()]\n",
    "        sumstats_df['hist_skew']=[s.skew()]\n",
    "        sumstats_df['hist_kurtosis']=[s.kurtosis()]\n",
    "        sumstats_df['hist_var']=[s.var()]\n",
    "        sumstats_df.columns = [prefix+col for col in sumstats_df.columns]\n",
    "        dfs.append(sumstats_df.reset_index())\n",
    "    \n",
    "    ###################################\n",
    "    #SUB-STEP 3\n",
    "    ###################################\n",
    "    CV_df = pd.DataFrame(list(zip(selection[\"XY_obj_cv_perbin\"].values[0][1:-1].split(\", \"))), columns =['CV']).astype(float)\n",
    "    sumstats_CV_df = pd.DataFrame()\n",
    "    sumstats_CV_df['XY_bin_CV_mean'] = CV_df.mean()\n",
    "    sumstats_CV_df['XY_bin_CV_median'] = CV_df.median()\n",
    "    sumstats_CV_df['XY_bin_CV_std'] = CV_df.std()\n",
    "    dfs.append(sumstats_CV_df.reset_index().drop(['index'], axis=1))\n",
    "    \n",
    "    ###################################\n",
    "    # Combine all resulting tables\n",
    "    ###################################\n",
    "    combined_df = pd.concat(dfs, axis=1).drop(columns=\"index\")\n",
    "    hist_dfs.append(combined_df)\n",
    "dist_org_summary = pd.concat(hist_dfs, ignore_index=True)\n",
    "dist_org_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- repeat the first two substeps for the nucleus distribution metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nucleus distribution\n",
    "nuc_dist_df = dist_df[[\"dataset\", \"image_name\", \n",
    "                    \"XY_bins\", \"XY_center_vox_cnt_perbin\", \"XY_mask_vox_cnt_perbin\",\n",
    "                    \"XY_wedges\", \"XY_center_vox_cnt_perwedge\", \"XY_mask_vox_cnt_perwedge\",\n",
    "                    \"Z_slices\", \"Z_center_vox_cnt\", \"Z_mask_vox_cnt\"]].set_index([\"dataset\", \"image_name\"])\n",
    "nuc_hist_dfs = []\n",
    "for idx in nuc_dist_df.index.unique():\n",
    "    selection = nuc_dist_df.loc[idx].iloc[[0]].reset_index()\n",
    "    bins_df = pd.DataFrame()\n",
    "    wedges_df = pd.DataFrame()\n",
    "    Z_df = pd.DataFrame()\n",
    "\n",
    "    bins_df[['bins', 'center', 'masks']] = selection[['XY_bins', 'XY_center_vox_cnt_perbin', 'XY_mask_vox_cnt_perbin']]\n",
    "    wedges_df[['bins', 'center', 'masks']] = selection[['XY_wedges', 'XY_center_vox_cnt_perwedge', 'XY_mask_vox_cnt_perwedge']]\n",
    "    Z_df[['bins', 'center', 'masks']] = selection[['Z_slices', 'Z_center_vox_cnt', 'Z_mask_vox_cnt']]\n",
    "\n",
    "    dfs = [selection[['dataset', 'image_name']]]\n",
    "    \n",
    "    for df, prefix in zip([bins_df, wedges_df, Z_df], [\"XY_bins_\", \"XY_wedges_\", \"Z_slices_\"]):\n",
    "        single_df = pd.DataFrame(list(zip(df[\"bins\"].values[0][1:-1].split(\", \"), \n",
    "                                        df[\"masks\"].values[0][1:-1].split(\", \"),\n",
    "                                        df[\"center\"].values[0][1:-1].split(\", \"))), columns =['bins', 'mask', 'obj']).astype(int)\n",
    "        \n",
    "        if \"Z_\" in prefix:\n",
    "            single_df =  single_df.drop(single_df[single_df['mask'] == 0].index)\n",
    "            single_df['bins'] = (single_df[\"bins\"]/max(single_df.bins)*9.99).apply(np.floor)+1\n",
    "            single_df = single_df.groupby(\"bins\").agg(['sum']).reset_index()\n",
    "            single_df.columns = ['bins',\"mask\",\"obj\"]\n",
    "\n",
    "        single_df['mask_fract'] = single_df['mask']/single_df['mask'].max()\n",
    "        # single_df['obj_normed_tocell'] = (single_df[\"obj\"]*single_df[\"mask_fract\"]).fillna(0)\n",
    "        single_df['obj_perc_per_bin'] = (single_df[\"obj\"] / single_df[\"obj\"].sum())*100\n",
    "        single_df['obj_portion_normed_tobin'] = (single_df[\"obj_perc_per_bin\"]/single_df[\"mask_fract\"]).fillna(0)\n",
    "\n",
    "        sumstats_df = pd.DataFrame()\n",
    "\n",
    "        s = single_df['bins'].repeat(single_df['obj_portion_normed_tobin']*100)\n",
    "        ###################################\n",
    "        #SUB-STEPS 1 & 2 FOR NUC\n",
    "        ###################################\n",
    "        sumstats_df['hist_mean']=[s.mean()]\n",
    "        sumstats_df['hist_median']=[s.median()]\n",
    "        if single_df['obj_portion_normed_tobin'].sum() != 0: sumstats_df['hist_mode']=[s.mode().iloc[0]]\n",
    "        else: sumstats_df['hist_mode']=['NaN']\n",
    "        sumstats_df['hist_min']=[s.min()]\n",
    "        sumstats_df['hist_max']=[s.max()]\n",
    "        sumstats_df['hist_range']=[s.max() - s.min()]\n",
    "        sumstats_df['hist_stdev']=[s.std()]\n",
    "        sumstats_df['hist_skew']=[s.skew()]\n",
    "        sumstats_df['hist_kurtosis']=[s.kurtosis()]\n",
    "        sumstats_df['hist_var']=[s.var()]\n",
    "        sumstats_df.columns = [prefix+col for col in sumstats_df.columns]\n",
    "        dfs.append(sumstats_df.reset_index())\n",
    "        \n",
    "    ###################################\n",
    "    # Combine all resulting tables\n",
    "    ###################################\n",
    "    combined_df = pd.concat(dfs, axis=1).drop(columns=\"index\")\n",
    "    nuc_hist_dfs.append(combined_df)\n",
    "dist_center_summary = pd.concat(nuc_hist_dfs, ignore_index=True)\n",
    "dist_center_summary.insert(2, column=\"object\", value=\"nuc\")\n",
    "dist_center_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- combine nucleus and organelle distribution tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_summary = pd.concat([dist_org_summary, dist_center_summary], axis=0).set_index(group_by).sort_index()\n",
    "dist_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`5` - Add normalized metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- calculate fraction of cell area taken up by the organelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# add normalization\n",
    "###################\n",
    "# organelle area fraction\n",
    "area_fractions = []\n",
    "for idx in org_summary.index.unique():\n",
    "    org_vol = org_summary.loc[idx][('volume', 'sum')]\n",
    "    cell_vol = regions_summary.loc[idx[:-1] + ('cell',)][\"volume\"]\n",
    "    afrac = org_vol/cell_vol\n",
    "    area_fractions.append(afrac)\n",
    "org_summary[('volume', 'fraction')] = area_fractions\n",
    "# TODO: add in line to reorder the level=0 columns here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- calculate fraction of organelle objects involved in specific interorganelle contacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contact sites volume normalized\n",
    "norm_toA_list = []\n",
    "norm_toB_list = []\n",
    "for col in contact_summary.index:\n",
    "    norm_toA_list.append(contact_summary.loc[col][('volume', 'sum')]/org_summary.loc[col[:-1]+(col[-1].split('X')[0],)][('volume', 'sum')])\n",
    "    norm_toB_list.append(contact_summary.loc[col][('volume', 'sum')]/org_summary.loc[col[:-1]+(col[-1].split('X')[1],)][('volume', 'sum')])\n",
    "contact_summary[('volume', 'norm_to_A')] = norm_toA_list\n",
    "contact_summary[('volume', 'norm_to_B')] = norm_toB_list\n",
    "\n",
    "# number and area of individuals organelle involved in contact\n",
    "cont_cnt = org_df[group_by]\n",
    "cont_cnt[[col.split('_')[0] for col in org_df.columns if col.endswith((\"_count\"))]] = org_df[[col for col in org_df.columns if col.endswith((\"_count\"))]].astype(bool)\n",
    "cont_cnt_perorg = cont_cnt.groupby(group_by).agg('sum')\n",
    "cont_cnt_perorg.columns = pd.MultiIndex.from_product([cont_cnt_perorg.columns, ['count_in']])\n",
    "for col in cont_cnt_perorg.columns:\n",
    "    cont_cnt_perorg[(col[0], 'num_fraction_in')] = cont_cnt_perorg[col].values/org_summary[('volume', 'count')].values\n",
    "cont_cnt_perorg.sort_index(axis=1, inplace=True)\n",
    "org_summary = pd.merge(org_summary, cont_cnt_perorg, on=group_by, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_cnt_perorg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`6` - Unstack and finalize summary stats tables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- unstack and reorder organelle morphology summary table columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# flatten datasheets and combine\n",
    "# TODO: restructure this so that all of the datasheets and unstacked and then reorded based on shared level 0 columns before flattening\n",
    "###################\n",
    "# org flattening\n",
    "org_final = org_summary.unstack(-1)\n",
    "for col in org_final.columns:\n",
    "    if col[1] in ('count_in', 'num_fraction_in') or col[0].endswith(('_count', '_volume')):\n",
    "        if col[2] not in col[0]:\n",
    "            org_final.drop(col,axis=1, inplace=True)\n",
    "new_col_order = ['dataset', 'image_name', 'object', 'volume', 'surface_area', 'SA_to_volume_ratio', \n",
    "                'equivalent_diameter', 'extent', 'euler_number', 'solidity', 'axis_major_length', \n",
    "                'ERXLD', 'ERXLD_count', 'ERXLD_volume', 'golgiXER', 'golgiXER_count', 'golgiXER_volume', \n",
    "                'golgiXLD', 'golgiXLD_count', 'golgiXLD_volume', 'golgiXperox', 'golgiXperox_count', 'golgiXperox_volume', \n",
    "                'lysoXER', 'lysoXER_count', 'lysoXER_volume', 'lysoXLD', 'lysoXLD_count', 'lysoXLD_volume', \n",
    "                'lysoXgolgi', 'lysoXgolgi_count', 'lysoXgolgi_volume', 'lysoXmito', 'lysoXmito_count', 'lysoXmito_volume', \n",
    "                'lysoXperox', 'lysoXperox_count', 'lysoXperox_volume', 'mitoXER', 'mitoXER_count', 'mitoXER_volume', \n",
    "                'mitoXLD', 'mitoXLD_count', 'mitoXLD_volume', 'mitoXgolgi', 'mitoXgolgi_count', 'mitoXgolgi_volume', \n",
    "                'mitoXperox', 'mitoXperox_count', 'mitoXperox_volume', 'peroxXER', 'peroxXER_count', 'peroxXER_volume', \n",
    "                'peroxXLD', 'peroxXLD_count', 'peroxXLD_volume']\n",
    "new_cols = org_final.columns.reindex(new_col_order, level=0)\n",
    "org_final = org_final.reindex(columns=new_cols[0])\n",
    "org_final.columns = [\"_\".join((col_name[-1], col_name[1], col_name[0])) for col_name in org_final.columns.to_flat_index()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fill \"NaN\" values with 0 when necessary to final organelle morphology summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming, filling \"NaN\" with 0 when needed, and removing ER_std columns\n",
    "for col in org_final.columns:\n",
    "    if '_count_in_' or '_fraction_in_' in col:\n",
    "        org_final[col] = org_final[col].fillna(0)\n",
    "    if col.endswith((\"_count_volume\",\"_sum_volume\", \"_mean_volume\", \"_median_volume\")):\n",
    "        org_final[col] = org_final[col].fillna(0)\n",
    "    if col.endswith(\"_count_volume\"):\n",
    "        org_final.rename(columns={col:col.split(\"_\")[0]+\"_count\"}, inplace=True)\n",
    "    if col.startswith(\"ER_std_\"):\n",
    "        org_final.drop(columns=[col], inplace=True)\n",
    "org_final = org_final.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- unstack and reorder organelle interactions summary table columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contacts flattened\n",
    "contact_final = contact_summary.unstack(-1)\n",
    "contact_final.columns = [\"_\".join((col_name[-1], col_name[1], col_name[0])) for col_name in contact_final.columns.to_flat_index()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fill \"NaN\" values with 0 when necessary to final organelle interactions summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renaming and filling \"NaN\" with 0 when needed\n",
    "for col in contact_final.columns:\n",
    "    if col.endswith((\"_count_volume\",\"_sum_volume\", \"_mean_volume\", \"_median_volume\")):\n",
    "        contact_final[col] = contact_final[col].fillna(0)\n",
    "    if col.endswith(\"_count_volume\"):\n",
    "        contact_final.rename(columns={col:col.split(\"_\")[0]+\"_count\"}, inplace=True)\n",
    "contact_final = contact_final.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- unstack and reorder distribution measurements summary table columns to create finalized table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributions flattened\n",
    "dist_final = dist_summary.unstack(-1)\n",
    "dist_final.columns = [\"_\".join((col_name[1], col_name[0])) for col_name in dist_final.columns.to_flat_index()]\n",
    "dist_final = dist_final.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- unstack and reorder region morphology summary table columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regions flattened\n",
    "regions_final = regions_summary.unstack(-1)\n",
    "regions_final.columns = [\"_\".join((col_name[1], col_name[0])) for col_name in regions_final.columns.to_flat_index()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- add normalization to finalize region morphology summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization added\n",
    "regions_final['nuc_area_fraction'] = regions_final['nuc_volume'] / regions_final['cell_volume']\n",
    "regions_final = regions_final.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- combine all four tables to create a complete summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining them all\n",
    "combined = pd.merge(org_final, contact_final, on=[\"dataset\", \"image_name\"], how=\"outer\")\n",
    "combined = pd.merge(combined, dist_final, on=[\"dataset\", \"image_name\"], how=\"outer\")\n",
    "combined = pd.merge(combined, regions_final, on=[\"dataset\", \"image_name\"], how=\"outer\").set_index([\"dataset\", \"image_name\"])\n",
    "combined.columns = [col.replace('sum', 'total') for col in combined.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`7` - Export summary stats tables as .csv files**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &#x1F6D1; &#x270D; **User Input Required:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# export summary sheets\n",
    "###################\n",
    "\n",
    "# All of the following options are correctly set to work with the sample data;\n",
    "# If you are not using the sample data, please edit the below as necessary.\n",
    "\n",
    "# location for the final csv files to be exported to\n",
    "# Change if not using sample data\n",
    "out_path = Path(os.getcwd()).parents[1] / \"sample_data\" /  \"batch_example\" / \"quant\"\n",
    "\n",
    "# prefix added to summary tables\n",
    "out_preffix = \"example_prototype_\"\n",
    "\n",
    "org_summary.to_csv(out_path + f\"/{out_preffix}per_org_summarystats.csv\")\n",
    "contact_summary.to_csv(out_path + f\"/{out_preffix}per_contact_summarystats.csv\")\n",
    "dist_summary.to_csv(out_path + f\"/{out_preffix}distribution_summarystats.csv\")\n",
    "regions_summary.to_csv(out_path + f\"/{out_preffix}per_region_summarystats.csv\")\n",
    "combined.to_csv(out_path + f\"/{out_preffix}summarystats_combined.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***EXECUTE FUNCTION PROTOTYPE***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define prototype `_batch_summary_stats` function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _batch_summary_stats(csv_path_list: List[str],\n",
    "                         out_path: str,\n",
    "                         out_preffix: str):\n",
    "    \"\"\"\" \n",
    "    csv_path_list: List[str],\n",
    "        A list of path strings where .csv files to analyze are located.\n",
    "    out_path: str,\n",
    "        A path string where the summary data file will be output to\n",
    "    out_preffix: str\n",
    "        The prefix used to name the output file.    \n",
    "    \"\"\"\n",
    "    ds_count = 0\n",
    "    fl_count = 0\n",
    "    ###################\n",
    "    # Read in the csv files and combine them into one of each type\n",
    "    ###################\n",
    "    org_tabs = []\n",
    "    contact_tabs = []\n",
    "    dist_tabs = []\n",
    "    region_tabs = []\n",
    "\n",
    "    for loc in csv_path_list:\n",
    "        ds_count = ds_count + 1\n",
    "        loc=Path(loc)\n",
    "        files_store = sorted(loc.glob(\"*.csv\"))\n",
    "        for file in files_store:\n",
    "            fl_count = fl_count + 1\n",
    "            stem = file.stem\n",
    "\n",
    "            org = \"organelles\"\n",
    "            contacts = \"contacts\"\n",
    "            dist = \"distributions\"\n",
    "            regions = \"_regions\"\n",
    "\n",
    "            if org in stem:\n",
    "                test_orgs = pd.read_csv(file, index_col=0)\n",
    "                test_orgs.insert(0, \"dataset\", stem[:-11])\n",
    "                org_tabs.append(test_orgs)\n",
    "            if contacts in stem:\n",
    "                test_contact = pd.read_csv(file, index_col=0)\n",
    "                test_contact.insert(0, \"dataset\", stem[:-9])\n",
    "                contact_tabs.append(test_contact)\n",
    "            if dist in stem:\n",
    "                test_dist = pd.read_csv(file, index_col=0)\n",
    "                test_dist.insert(0, \"dataset\", stem[:-14])\n",
    "                dist_tabs.append(test_dist)\n",
    "            if regions in stem:\n",
    "                test_regions = pd.read_csv(file, index_col=0)\n",
    "                test_regions.insert(0, \"dataset\", stem[:-8])\n",
    "                region_tabs.append(test_regions)\n",
    "            \n",
    "    org_df = pd.concat(org_tabs,axis=0, join='outer')\n",
    "    contacts_df = pd.concat(contact_tabs,axis=0, join='outer')\n",
    "    dist_df = pd.concat(dist_tabs,axis=0, join='outer')\n",
    "    regions_df = pd.concat(region_tabs,axis=0, join='outer')\n",
    "\n",
    "    ###################\n",
    "    # adding new metrics to the original sheets\n",
    "    ###################\n",
    "    # TODO: include these labels when creating the original sheets\n",
    "    contact_cnt = contacts_df[[\"dataset\", \"image_name\", \"object\", \"label\", \"volume\"]]\n",
    "    contact_cnt[[\"orgA\", \"orgB\"]] = contact_cnt[\"object\"].str.split('X', expand=True)\n",
    "    contact_cnt[[\"A_ID\", \"B_ID\"]] = contact_cnt[\"label\"].str.split('_', expand=True)\n",
    "    contact_cnt[\"A\"] = contact_cnt[\"orgA\"] +\"_\" + contact_cnt[\"A_ID\"].astype(str)\n",
    "    contact_cnt[\"B\"] = contact_cnt[\"orgB\"] +\"_\" + contact_cnt[\"B_ID\"].astype(str)\n",
    "\n",
    "    contact_cnt_percell = contact_cnt[[\"dataset\", \"image_name\", \"orgA\", \"A_ID\", \"object\", \"volume\"]].groupby([\"dataset\", \"image_name\", \"orgA\", \"A_ID\", \"object\"]).agg([\"count\", \"sum\"])\n",
    "    contact_cnt_percell.columns = [\"_\".join(col_name).rstrip('_') for col_name in contact_cnt_percell.columns.to_flat_index()]\n",
    "    unstacked = contact_cnt_percell.unstack(level='object')\n",
    "    unstacked.columns = [\"_\".join(col_name).rstrip('_') for col_name in unstacked.columns.to_flat_index()]\n",
    "    unstacked = unstacked.reset_index()\n",
    "    for col in unstacked.columns:\n",
    "        if col.startswith(\"volume_count_\"):\n",
    "            newname = col.split(\"_\")[-1] + \"_count\"\n",
    "            unstacked.rename(columns={col:newname}, inplace=True)\n",
    "        if col.startswith(\"volume_sum_\"):\n",
    "            newname = col.split(\"_\")[-1] + \"_volume\"\n",
    "            unstacked.rename(columns={col:newname}, inplace=True)\n",
    "    unstacked.rename(columns={\"orgA\":\"object\", \"A_ID\":\"label\"}, inplace=True)\n",
    "    unstacked.set_index(['dataset', 'image_name', 'object', 'label'])\n",
    "\n",
    "    contact_percellB = contact_cnt[[\"dataset\", \"image_name\", \"orgB\", \"B_ID\", \"object\", \"volume\"]].groupby([\"dataset\", \"image_name\", \"orgB\", \"B_ID\", \"object\"]).agg([\"count\", \"sum\"])\n",
    "    contact_percellB.columns = [\"_\".join(col_name).rstrip('_') for col_name in contact_percellB.columns.to_flat_index()]\n",
    "    unstackedB = contact_percellB.unstack(level='object')\n",
    "    unstackedB.columns = [\"_\".join(col_name).rstrip('_') for col_name in unstackedB.columns.to_flat_index()]\n",
    "    unstackedB = unstackedB.reset_index()\n",
    "    for col in unstackedB.columns:\n",
    "        if col.startswith(\"volume_count_\"):\n",
    "            newname = col.split(\"_\")[-1] + \"_count\"\n",
    "            unstackedB.rename(columns={col:newname}, inplace=True)\n",
    "        if col.startswith(\"volume_sum_\"):\n",
    "            newname = col.split(\"_\")[-1] + \"_volume\"\n",
    "            unstackedB.rename(columns={col:newname}, inplace=True)\n",
    "    unstackedB.rename(columns={\"orgB\":\"object\", \"B_ID\":\"label\"}, inplace=True)\n",
    "    unstackedB.set_index(['dataset', 'image_name', 'object', 'label'])\n",
    "\n",
    "    contact_cnt = pd.concat([unstacked, unstackedB], axis=0).sort_index(axis=0)\n",
    "    contact_cnt = contact_cnt.groupby(['dataset', 'image_name', 'object', 'label']).sum().reset_index()\n",
    "    contact_cnt['label']=contact_cnt['label'].astype(\"Int64\")\n",
    "\n",
    "    org_df = pd.merge(org_df, contact_cnt, how='left', on=['dataset', 'image_name', 'object', 'label'], sort=True)\n",
    "    org_df[contact_cnt.columns] = org_df[contact_cnt.columns].fillna(0)\n",
    "\n",
    "    ###################\n",
    "    # summary stat group\n",
    "    ###################\n",
    "    group_by = ['dataset', 'image_name', 'object']\n",
    "    sharedcolumns = [\"SA_to_volume_ratio\", \"equivalent_diameter\", \"extent\", \"euler_number\", \"solidity\", \"axis_major_length\"]\n",
    "    ag_func_standard = ['mean', 'median', 'std']\n",
    "\n",
    "    ###################\n",
    "    # summarize shared measurements between org_df and contacts_df\n",
    "    ###################\n",
    "    org_cont_tabs = []\n",
    "    for tab in [org_df, contacts_df]:\n",
    "        tab1 = tab[group_by + ['volume']].groupby(group_by).agg(['count', 'sum'] + ag_func_standard)\n",
    "        tab2 = tab[group_by + ['surface_area']].groupby(group_by).agg(['sum'] + ag_func_standard)\n",
    "        tab3 = tab[group_by + sharedcolumns].groupby(group_by).agg(ag_func_standard)\n",
    "        shared_metrics = pd.merge(tab1, tab2, 'outer', on=group_by)\n",
    "        shared_metrics = pd.merge(shared_metrics, tab3, 'outer', on=group_by)\n",
    "        org_cont_tabs.append(shared_metrics)\n",
    "\n",
    "    org_summary = org_cont_tabs[0]\n",
    "    contact_summary = org_cont_tabs[1]\n",
    "\n",
    "    ###################\n",
    "    # group metrics from regions_df similar to the above\n",
    "    ###################\n",
    "    regions_summary = regions_df[group_by + ['volume', 'surface_area'] + sharedcolumns].set_index(group_by)\n",
    "\n",
    "    ###################\n",
    "    # summarize extra metrics from org_df\n",
    "    ###################\n",
    "    columns2 = [col for col in org_df.columns if col.endswith((\"_count\", \"_volume\"))]\n",
    "    contact_counts_summary = org_df[group_by + columns2].groupby(group_by).agg(['sum'] + ag_func_standard)\n",
    "    org_summary = pd.merge(org_summary, contact_counts_summary, 'outer', on=group_by)#left_on=group_by, right_on=True)\n",
    "\n",
    "    ###################\n",
    "    # summarize distribution measurements\n",
    "    ###################\n",
    "    # organelle distributions\n",
    "    hist_dfs = []\n",
    "    for ind in range(0,len(dist_df.index)):\n",
    "        selection = dist_df.iloc[[ind]] #    selection = dist_df.loc[[ind]]\n",
    "        bins_df = pd.DataFrame()\n",
    "        wedges_df = pd.DataFrame()\n",
    "        Z_df = pd.DataFrame()\n",
    "        CV_df = pd.DataFrame()\n",
    "\n",
    "        bins_df[['bins', 'masks', 'obj']] = selection[['XY_bins', 'XY_mask_vox_cnt_perbin', 'XY_obj_vox_cnt_perbin']]\n",
    "        wedges_df[['bins', 'masks', 'obj']] = selection[['XY_wedges', 'XY_mask_vox_cnt_perwedge', 'XY_obj_vox_cnt_perwedge']]\n",
    "        Z_df[['bins', 'masks', 'obj']] = selection[['Z_slices', 'Z_mask_vox_cnt', 'Z_obj_vox_cnt']]\n",
    "\n",
    "        dfs = [selection[['dataset', 'image_name', 'object']].reset_index()]\n",
    "        for df, prefix in zip([bins_df, wedges_df, Z_df], [\"XY_bins_\", \"XY_wedges_\", \"Z_slices_\"]):\n",
    "            single_df = pd.DataFrame(list(zip(df[\"bins\"].values[0][1:-1].split(\", \"), \n",
    "                                            df[\"obj\"].values[0][1:-1].split(\", \"), \n",
    "                                            df[\"masks\"].values[0][1:-1].split(\", \"))), columns =['bins', 'obj', 'mask']).astype(int)\n",
    "            \n",
    "            if \"Z_\" in prefix:\n",
    "                single_df =  single_df.drop(single_df[single_df['mask'] == 0].index)\n",
    "                single_df['bins'] = (single_df[\"bins\"]/max(single_df.bins)*9.99).apply(np.floor)+1\n",
    "                single_df = single_df.groupby(\"bins\").agg(['sum']).reset_index()\n",
    "                single_df.columns = ['bins',\"obj\",\"mask\"]\n",
    "        \n",
    "            single_df['mask_fract'] = single_df['mask']/single_df['mask'].max()\n",
    "            # single_df['obj_normed_tocell'] = (single_df[\"obj\"]*single_df[\"mask_fract\"]).fillna(0)\n",
    "            single_df['obj_perc_per_bin'] = (single_df[\"obj\"] / single_df[\"obj\"].sum())*100\n",
    "            single_df['obj_portion_normed_tobin'] = (single_df[\"obj_perc_per_bin\"]/single_df[\"mask_fract\"]).fillna(0)\n",
    "\n",
    "            sumstats_df = pd.DataFrame()\n",
    "\n",
    "            s = single_df['bins'].repeat(single_df['obj_portion_normed_tobin']*100)\n",
    "\n",
    "            sumstats_df['hist_mean']=[s.mean()]\n",
    "            sumstats_df['hist_median']=[s.median()]\n",
    "            if single_df['obj_portion_normed_tobin'].sum() != 0: sumstats_df['hist_mode']=[s.mode().iloc[0]]\n",
    "            else: sumstats_df['hist_mode']=['NaN']\n",
    "            sumstats_df['hist_min']=[s.min()]\n",
    "            sumstats_df['hist_max']=[s.max()]\n",
    "            sumstats_df['hist_range']=[s.max() - s.min()]\n",
    "            sumstats_df['hist_stdev']=[s.std()]\n",
    "            sumstats_df['hist_skew']=[s.skew()]\n",
    "            sumstats_df['hist_kurtosis']=[s.kurtosis()]\n",
    "            sumstats_df['hist_var']=[s.var()]\n",
    "            sumstats_df.columns = [prefix+col for col in sumstats_df.columns]\n",
    "            dfs.append(sumstats_df.reset_index())\n",
    "\n",
    "        CV_df = pd.DataFrame(list(zip(selection[\"XY_obj_cv_perbin\"].values[0][1:-1].split(\", \"))), columns =['CV']).astype(float)\n",
    "        sumstats_CV_df = pd.DataFrame()\n",
    "        sumstats_CV_df['XY_bin_CV_mean'] = CV_df.mean()\n",
    "        sumstats_CV_df['XY_bin_CV_median'] = CV_df.median()\n",
    "        sumstats_CV_df['XY_bin_CV_std'] = CV_df.std()\n",
    "        dfs.append(sumstats_CV_df.reset_index().drop(['index'], axis=1))\n",
    "\n",
    "        combined_df = pd.concat(dfs, axis=1).drop(columns=\"index\")\n",
    "        hist_dfs.append(combined_df)\n",
    "    dist_org_summary = pd.concat(hist_dfs, ignore_index=True)\n",
    "    dist_org_summary\n",
    "\n",
    "    # nucleus distribution\n",
    "    nuc_dist_df = dist_df[[\"dataset\", \"image_name\", \n",
    "                        \"XY_bins\", \"XY_center_vox_cnt_perbin\", \"XY_mask_vox_cnt_perbin\",\n",
    "                        \"XY_wedges\", \"XY_center_vox_cnt_perwedge\", \"XY_mask_vox_cnt_perwedge\",\n",
    "                        \"Z_slices\", \"Z_center_vox_cnt\", \"Z_mask_vox_cnt\"]].set_index([\"dataset\", \"image_name\"])\n",
    "    nuc_hist_dfs = []\n",
    "    for idx in nuc_dist_df.index.unique():\n",
    "        selection = nuc_dist_df.loc[idx].iloc[[0]].reset_index()\n",
    "        bins_df = pd.DataFrame()\n",
    "        wedges_df = pd.DataFrame()\n",
    "        Z_df = pd.DataFrame()\n",
    "\n",
    "        bins_df[['bins', 'center', 'masks']] = selection[['XY_bins', 'XY_center_vox_cnt_perbin', 'XY_mask_vox_cnt_perbin']]\n",
    "        wedges_df[['bins', 'center', 'masks']] = selection[['XY_wedges', 'XY_center_vox_cnt_perwedge', 'XY_mask_vox_cnt_perwedge']]\n",
    "        Z_df[['bins', 'center', 'masks']] = selection[['Z_slices', 'Z_center_vox_cnt', 'Z_mask_vox_cnt']]\n",
    "\n",
    "        dfs = [selection[['dataset', 'image_name']]]\n",
    "        for df, prefix in zip([bins_df, wedges_df, Z_df], [\"XY_bins_\", \"XY_wedges_\", \"Z_slices_\"]):\n",
    "            single_df = pd.DataFrame(list(zip(df[\"bins\"].values[0][1:-1].split(\", \"), \n",
    "                                            df[\"masks\"].values[0][1:-1].split(\", \"),\n",
    "                                            df[\"center\"].values[0][1:-1].split(\", \"))), columns =['bins', 'mask', 'obj']).astype(int)\n",
    "\n",
    "            if \"Z_\" in prefix:\n",
    "                single_df =  single_df.drop(single_df[single_df['mask'] == 0].index)\n",
    "                single_df['bins'] = (single_df[\"bins\"]/max(single_df.bins)*9.99).apply(np.floor)+1\n",
    "                single_df = single_df.groupby(\"bins\").agg(['sum']).reset_index()\n",
    "                single_df.columns = ['bins',\"mask\",\"obj\"]\n",
    "        \n",
    "            single_df['mask_fract'] = single_df['mask']/single_df['mask'].max()\n",
    "            # single_df['obj_normed_tocell'] = (single_df[\"obj\"]*single_df[\"mask_fract\"]).fillna(0)\n",
    "            single_df['obj_perc_per_bin'] = (single_df[\"obj\"] / single_df[\"obj\"].sum())*100\n",
    "            single_df['obj_portion_normed_tobin'] = (single_df[\"obj_perc_per_bin\"]/single_df[\"mask_fract\"]).fillna(0)\n",
    "\n",
    "            sumstats_df = pd.DataFrame()\n",
    "\n",
    "            s = single_df['bins'].repeat(single_df['obj_portion_normed_tobin']*100)\n",
    "\n",
    "            sumstats_df['hist_mean']=[s.mean()]\n",
    "            sumstats_df['hist_median']=[s.median()]\n",
    "            if single_df['obj_portion_normed_tobin'].sum() != 0: sumstats_df['hist_mode']=[s.mode().iloc[0]]\n",
    "            else: sumstats_df['hist_mode']=['NaN']\n",
    "            sumstats_df['hist_min']=[s.min()]\n",
    "            sumstats_df['hist_max']=[s.max()]\n",
    "            sumstats_df['hist_range']=[s.max() - s.min()]\n",
    "            sumstats_df['hist_stdev']=[s.std()]\n",
    "            sumstats_df['hist_skew']=[s.skew()]\n",
    "            sumstats_df['hist_kurtosis']=[s.kurtosis()]\n",
    "            sumstats_df['hist_var']=[s.var()]\n",
    "            sumstats_df.columns = [prefix+col for col in sumstats_df.columns]\n",
    "            dfs.append(sumstats_df.reset_index())\n",
    "        combined_df = pd.concat(dfs, axis=1).drop(columns=\"index\")\n",
    "        nuc_hist_dfs.append(combined_df)\n",
    "    dist_center_summary = pd.concat(nuc_hist_dfs, ignore_index=True)\n",
    "    dist_center_summary.insert(2, column=\"object\", value=\"nuc\")\n",
    "\n",
    "    dist_summary = pd.concat([dist_org_summary, dist_center_summary], axis=0).set_index(group_by).sort_index()\n",
    "\n",
    "\n",
    "    ###################\n",
    "    # add normalization\n",
    "    ###################\n",
    "    # organelle area fraction\n",
    "    area_fractions = []\n",
    "    for idx in org_summary.index.unique():\n",
    "        org_vol = org_summary.loc[idx][('volume', 'sum')]\n",
    "        cell_vol = regions_summary.loc[idx[:-1] + ('cell',)][\"volume\"]\n",
    "        afrac = org_vol/cell_vol\n",
    "        area_fractions.append(afrac)\n",
    "    org_summary[('volume', 'fraction')] = area_fractions\n",
    "    # TODO: add in line to reorder the level=0 columns here\n",
    "\n",
    "    # contact sites volume normalized\n",
    "    norm_toA_list = []\n",
    "    norm_toB_list = []\n",
    "    for col in contact_summary.index:\n",
    "        norm_toA_list.append(contact_summary.loc[col][('volume', 'sum')]/org_summary.loc[col[:-1]+(col[-1].split('X')[0],)][('volume', 'sum')])\n",
    "        norm_toB_list.append(contact_summary.loc[col][('volume', 'sum')]/org_summary.loc[col[:-1]+(col[-1].split('X')[1],)][('volume', 'sum')])\n",
    "    contact_summary[('volume', 'norm_to_A')] = norm_toA_list\n",
    "    contact_summary[('volume', 'norm_to_B')] = norm_toB_list\n",
    "\n",
    "    # number and area of individuals organelle involved in contact\n",
    "    cont_cnt = org_df[group_by]\n",
    "    cont_cnt[[col.split('_')[0] for col in org_df.columns if col.endswith((\"_count\"))]] = org_df[[col for col in org_df.columns if col.endswith((\"_count\"))]].astype(bool)\n",
    "    cont_cnt_perorg = cont_cnt.groupby(group_by).agg('sum')\n",
    "    cont_cnt_perorg.columns = pd.MultiIndex.from_product([cont_cnt_perorg.columns, ['count_in']])\n",
    "    for col in cont_cnt_perorg.columns:\n",
    "        cont_cnt_perorg[(col[0], 'num_fraction_in')] = cont_cnt_perorg[col].values/org_summary[('volume', 'count')].values\n",
    "    cont_cnt_perorg.sort_index(axis=1, inplace=True)\n",
    "    org_summary = pd.merge(org_summary, cont_cnt_perorg, on=group_by, how='outer')\n",
    "\n",
    "\n",
    "    ###################\n",
    "    # flatten datasheets and combine\n",
    "    # TODO: restructure this so that all of the datasheets and unstacked and then reorded based on shared level 0 columns before flattening\n",
    "    ###################\n",
    "    # org flattening\n",
    "    org_final = org_summary.unstack(-1)\n",
    "    for col in org_final.columns:\n",
    "        if col[1] in ('count_in', 'num_fraction_in') or col[0].endswith(('_count', '_volume')):\n",
    "            if col[2] not in col[0]:\n",
    "                org_final.drop(col,axis=1, inplace=True)\n",
    "    new_col_order = ['dataset', 'image_name', 'object', 'volume', 'surface_area', 'SA_to_volume_ratio', \n",
    "                 'equivalent_diameter', 'extent', 'euler_number', 'solidity', 'axis_major_length', \n",
    "                 'ERXLD', 'ERXLD_count', 'ERXLD_volume', 'golgiXER', 'golgiXER_count', 'golgiXER_volume', \n",
    "                 'golgiXLD', 'golgiXLD_count', 'golgiXLD_volume', 'golgiXperox', 'golgiXperox_count', 'golgiXperox_volume', \n",
    "                 'lysoXER', 'lysoXER_count', 'lysoXER_volume', 'lysoXLD', 'lysoXLD_count', 'lysoXLD_volume', \n",
    "                 'lysoXgolgi', 'lysoXgolgi_count', 'lysoXgolgi_volume', 'lysoXmito', 'lysoXmito_count', 'lysoXmito_volume', \n",
    "                 'lysoXperox', 'lysoXperox_count', 'lysoXperox_volume', 'mitoXER', 'mitoXER_count', 'mitoXER_volume', \n",
    "                 'mitoXLD', 'mitoXLD_count', 'mitoXLD_volume', 'mitoXgolgi', 'mitoXgolgi_count', 'mitoXgolgi_volume', \n",
    "                 'mitoXperox', 'mitoXperox_count', 'mitoXperox_volume', 'peroxXER', 'peroxXER_count', 'peroxXER_volume', \n",
    "                 'peroxXLD', 'peroxXLD_count', 'peroxXLD_volume']\n",
    "    new_cols = org_final.columns.reindex(new_col_order, level=0)\n",
    "    org_final = org_final.reindex(columns=new_cols[0])\n",
    "    org_final.columns = [\"_\".join((col_name[-1], col_name[1], col_name[0])) for col_name in org_final.columns.to_flat_index()]\n",
    "\n",
    "    #renaming, filling \"NaN\" with 0 when needed, and removing ER_std columns\n",
    "    for col in org_final.columns:\n",
    "        if '_count_in_' or '_fraction_in_' in col:\n",
    "            org_final[col] = org_final[col].fillna(0)\n",
    "        if col.endswith((\"_count_volume\",\"_sum_volume\", \"_mean_volume\", \"_median_volume\")):\n",
    "            org_final[col] = org_final[col].fillna(0)\n",
    "        if col.endswith(\"_count_volume\"):\n",
    "            org_final.rename(columns={col:col.split(\"_\")[0]+\"_count\"}, inplace=True)\n",
    "        if col.startswith(\"ER_std_\"):\n",
    "            org_final.drop(columns=[col], inplace=True)\n",
    "    org_final = org_final.reset_index()\n",
    "\n",
    "    # contacts flattened\n",
    "    contact_final = contact_summary.unstack(-1)\n",
    "    contact_final.columns = [\"_\".join((col_name[-1], col_name[1], col_name[0])) for col_name in contact_final.columns.to_flat_index()]\n",
    "\n",
    "    #renaming and filling \"NaN\" with 0 when needed\n",
    "    for col in contact_final.columns:\n",
    "        if col.endswith((\"_count_volume\",\"_sum_volume\", \"_mean_volume\", \"_median_volume\")):\n",
    "            contact_final[col] = contact_final[col].fillna(0)\n",
    "        if col.endswith(\"_count_volume\"):\n",
    "            contact_final.rename(columns={col:col.split(\"_\")[0]+\"_count\"}, inplace=True)\n",
    "    contact_final = contact_final.reset_index()\n",
    "\n",
    "    # distributions flattened\n",
    "    dist_final = dist_summary.unstack(-1)\n",
    "    dist_final.columns = [\"_\".join((col_name[1], col_name[0])) for col_name in dist_final.columns.to_flat_index()]\n",
    "    dist_final = dist_final.reset_index()\n",
    "\n",
    "    # regions flattened & normalization added\n",
    "    regions_final = regions_summary.unstack(-1)\n",
    "    regions_final.columns = [\"_\".join((col_name[1], col_name[0])) for col_name in regions_final.columns.to_flat_index()]\n",
    "    regions_final['nuc_area_fraction'] = regions_final['nuc_volume'] / regions_final['cell_volume']\n",
    "    regions_final = regions_final.reset_index()\n",
    "\n",
    "    # combining them all\n",
    "    combined = pd.merge(org_final, contact_final, on=[\"dataset\", \"image_name\"], how=\"outer\")\n",
    "    combined = pd.merge(combined, dist_final, on=[\"dataset\", \"image_name\"], how=\"outer\")\n",
    "    combined = pd.merge(combined, regions_final, on=[\"dataset\", \"image_name\"], how=\"outer\").set_index([\"dataset\", \"image_name\"])\n",
    "    combined.columns = [col.replace('sum', 'total') for col in combined.columns]\n",
    "\n",
    "    ###################\n",
    "    # export summary sheets\n",
    "    ###################\n",
    "    org_summary.to_csv(out_path + f\"/{out_preffix}per_org_summarystats.csv\")\n",
    "    contact_summary.to_csv(out_path + f\"/{out_preffix}per_contact_summarystats.csv\")\n",
    "    dist_summary.to_csv(out_path + f\"/{out_preffix}distribution_summarystats.csv\")\n",
    "    regions_summary.to_csv(out_path + f\"/{out_preffix}per_region_summarystats.csv\")\n",
    "    combined.to_csv(out_path + f\"/{out_preffix}summarystats_combined.csv\")\n",
    "\n",
    "    print(f\"Processing of {fl_count} files from {ds_count} dataset(s) is complete.\")\n",
    "    return f\"{fl_count} files from {ds_count} dataset(s) were processed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Run prototype `_batch_summary_stats` function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &#x1F6D1; &#x270D; **User Input Required:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the following options are correctly set to work with the sample data;\n",
    "# If you are not using the sample data, please edit the below as necessary.\n",
    "\n",
    "out_path = Path(os.getcwd()).parents[1] / \"sample_data\" /  \"batch_example\" / \"quant\",\n",
    "out_preffix = \"example_prototype_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &#x1F3C3; **Run code; no user input required**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=_batch_summary_stats(csv_path_list = csv_path_list,\n",
    "                         out_path = out_path,\n",
    "                         out_preffix = out_preffix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infer-subc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# &#x1F50D; Checking segmentation outputs from Organelle-Segmenter-Plugin\n",
    "\n",
    "### &#x1F4D6; **How to:** \n",
    "\n",
    "Advance through each block of code sequentially by pressing `Shift`+`Enter`.\n",
    "\n",
    "If a block of code contains &#x1F53D; follow the written instructions to fill in the blanks below that line before running it.\n",
    "```python\n",
    "#### USER INPUT REQUIRED ###\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________\n",
    "## \t**INPUTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &#x1F3C3; **Run code; no user input required**\n",
    "\n",
    "&#x1F453; **FYI:** This code block loads all of the necessary python packages and functions you will need for this notebook. Additionally, a [Napari](https://napari.org/stable/) window will open; this is where you will be able to visual the segmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top level imports\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "from typing import Optional, Union, Dict, List\n",
    "import itertools \n",
    "import glob\n",
    "\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import napari\n",
    "\n",
    "### import local python functions in ../infer_subc\n",
    "sys.path.append(os.path.abspath((os.path.join(os.getcwd(), '..'))))\n",
    "\n",
    "from infer_subc.core.file_io import (read_czi_image,\n",
    "                                        export_inferred_organelle,\n",
    "                                        import_inferred_organelle,\n",
    "                                        export_tiff,\n",
    "                                        list_image_files,\n",
    "                                        read_tiff_image)\n",
    "\n",
    "\n",
    "\n",
    "from infer_subc.constants import *\n",
    "from infer_subc.utils.stats import *\n",
    "from infer_subc.utils.stats_helpers import *\n",
    "from infer_subc.utils.stats import _assert_uint16_labels\n",
    "from infer_subc.core.img import label_uint16\n",
    "\n",
    "\n",
    "import time\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "viewer = napari.Viewer()\n",
    "\n",
    "# TODO: include the file_type option in the main import_inferred_organelle() function\n",
    "def _import_inferred_organelle(name: str, meta_dict: Dict, out_data_path: Path, file_type: str) -> Union[np.ndarray, None]:\n",
    "    \"\"\"\n",
    "    read inferred organelle from ome.tif file\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    name: str\n",
    "        name of organelle.  i.e. nuc, lyso, etc.\n",
    "    meta_dict:\n",
    "        dictionary of meta-data (ome) from original file\n",
    "    out_data_path:\n",
    "        Path object of directory where tiffs are read from\n",
    "    file_type: \n",
    "        The type of file you want to import as a string (ex - \".tif\", \".tiff\", \".czi\", etc.)\n",
    "\n",
    "    Returns\n",
    "    -------------\n",
    "    exported file name\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # copy the original file name to meta\n",
    "    img_name = Path(meta_dict[\"file_name\"])  #\n",
    "    # add params to metadata\n",
    "    if name is None:\n",
    "        pass\n",
    "    else:\n",
    "        organelle_fname = f\"{img_name.stem}-{name}{file_type}\"\n",
    "\n",
    "        organelle_path = out_data_path / organelle_fname\n",
    "\n",
    "        if Path.exists(organelle_path):\n",
    "            # organelle_obj, _meta_dict = read_ome_image(organelle_path)\n",
    "            organelle_obj = read_tiff_image(organelle_path)  # .squeeze()\n",
    "            print(f\"loaded  inferred {len(organelle_obj.shape)}D `{name}`  from {out_data_path} \")\n",
    "            return organelle_obj\n",
    "        else:\n",
    "            print(f\"`{name}` object not found: {organelle_path}\")\n",
    "            raise FileNotFoundError(f\"`{name}` object not found: {organelle_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________\n",
    "## **QUALITY CHECK OF SEGMENTATIONS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &#x1F6D1; &#x270D; **User Input Required:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### USER INPUT REQUIRED ###\n",
    "# Copy and paste the paths to the folders where your data is saved inside the quotation marks below. \n",
    "# If you have more than one segmentation data folder, include it in the segmentation_data_2 line. If not, type None wihtout quotation marks\n",
    "# NOTE: for windows, use \"/\" \n",
    "raw_data = \"D:/Experiments (C2-117 - current)/C2-121/C2-121_deconvolution\"\n",
    "segmentation_data = \"D:/Experiments (C2-117 - current)/C2-121/20230921_C2-121_3D-analysis/20230921_C2-121_segmentation\"\n",
    "\n",
    "location_tosave_edited_segmentations = \"D:/Experiments (C2-117 - current)/C2-121/20230921_C2-121_3D-analysis/20240102_C2-121_segmentation-edits\"\n",
    "location_tosave_fullset_gooddata = \"D:/Experiments (C2-117 - current)/C2-121/20230921_C2-121_3D-analysis/C2-121_good-segs\"\n",
    "\n",
    "# In quotation marks, include the extension of the file type for your SEGMENTATION and RAW images\n",
    "raw_file_type = \".tiff\"\n",
    "seg_file_type = \".tiff\"\n",
    "\n",
    "# In quotation marks, write the suffix associated to each segmentation file. If you don't have that image \n",
    "mask_suffix = \"masks_A\"\n",
    "lyso_suffix = \"lyso\"\n",
    "mito_suffix = \"mito\"\n",
    "golgi_suffix = \"golgi\"\n",
    "perox_suffix = \"perox\"\n",
    "ER_suffix = \"ER\"\n",
    "LD_suffix = \"LD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Optional - USER INPUT REQUIRED ###\n",
    "# If your segmentations are saved in more than one folder, fill in the information below about the second file location. If not, type None wihtout quotation marks in all of the lines below.\n",
    "# Copy and paste the paths to the folders where your data is saved inside the quotation marks below. \n",
    "segmentation_data_2 = None\n",
    "\n",
    "# In quotation marks, write the suffix associated to each segmentation file; if \n",
    "mask_suffix_2 = None\n",
    "lyso_suffix_2 = None\n",
    "mito_suffix_2 = None\n",
    "golgi_suffix_2 = None\n",
    "perox_suffix_2 = None\n",
    "ER_suffix_2 = None\n",
    "LD_suffix_2 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#x1F3C3; **Run code; no user input required**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 2_cell 1_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 2_cell 2_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 2_cell 3_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 2_cell 4_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 2_cell 5_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 3_cell 1_25nM TG_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 3_cell 2_25nM TG_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 3_cell 3_25nM TG_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 3_cell 4_25nM TG_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 3_cell 5_25nM TG_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 4_cell 1_untreated_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 4_cell 2_untreated_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 4_cell 3_untreated_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 4_cell 4_untreated_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 4_cell 5_untreated_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 5_cell 1_50uM NaAsO  washout_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 5_cell 2_50uM NaAsO  washout_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 5_cell 3_50uM NaAsO  washout_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 5_cell 4_50uM NaAsO  washout_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 5_cell 5_50uM NaAsO  washout_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 6_cell 1_25nM TG_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 6_cell 2_25nM TG_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 6_cell 3_25nM TG_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 6_cell 4_25nM TG_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 6_cell 5_25nM TG_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 7_cell 1_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 7_cell 2_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 7_cell 3_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 7_cell 4_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 7_cell 5_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 8_cell 1_untreated_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 8_cell 2_untreated_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 8_cell 3_untreated_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 8_cell 4_untreated_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 10_cell 1_25nM TG_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 10_cell 2_25nM TG_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 10_cell 3_25nM TG_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 10_cell 4_25nM TG_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 10_cell 5_25nM TG_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 11_cell 1_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 11_cell 2_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 11_cell 3_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 11_cell 4_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 11_cell 5_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 12_cell 1_untreated_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 12_cell 2_untreated_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 12_cell 3_untreated_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 12_cell 4_untreated_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 9_cell 1_50uM NaAsO  washout_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 9_cell 2_50uM NaAsO  washout_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 9_cell 3_50uM NaAsO  washout_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 9_cell 4_50uM NaAsO  washout_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 9_cell 5_50uM NaAsO  washout_Linear unmixing_0_cmle.ome.tiff</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                       Image Name\n",
       "0              D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 2_cell 1_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff\n",
       "1              D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 2_cell 2_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff\n",
       "2              D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 2_cell 3_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff\n",
       "3              D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 2_cell 4_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff\n",
       "4              D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 2_cell 5_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff\n",
       "5                 D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 3_cell 1_25nM TG_Linear unmixing_0_cmle.ome.tiff\n",
       "6                 D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 3_cell 2_25nM TG_Linear unmixing_0_cmle.ome.tiff\n",
       "7                 D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 3_cell 3_25nM TG_Linear unmixing_0_cmle.ome.tiff\n",
       "8                 D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 3_cell 4_25nM TG_Linear unmixing_0_cmle.ome.tiff\n",
       "9                 D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 3_cell 5_25nM TG_Linear unmixing_0_cmle.ome.tiff\n",
       "10              D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 4_cell 1_untreated_Linear unmixing_0_cmle.ome.tiff\n",
       "11              D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 4_cell 2_untreated_Linear unmixing_0_cmle.ome.tiff\n",
       "12              D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 4_cell 3_untreated_Linear unmixing_0_cmle.ome.tiff\n",
       "13              D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 4_cell 4_untreated_Linear unmixing_0_cmle.ome.tiff\n",
       "14              D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 4_cell 5_untreated_Linear unmixing_0_cmle.ome.tiff\n",
       "15    D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 5_cell 1_50uM NaAsO  washout_Linear unmixing_0_cmle.ome.tiff\n",
       "16    D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 5_cell 2_50uM NaAsO  washout_Linear unmixing_0_cmle.ome.tiff\n",
       "17    D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 5_cell 3_50uM NaAsO  washout_Linear unmixing_0_cmle.ome.tiff\n",
       "18    D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 5_cell 4_50uM NaAsO  washout_Linear unmixing_0_cmle.ome.tiff\n",
       "19    D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 5_cell 5_50uM NaAsO  washout_Linear unmixing_0_cmle.ome.tiff\n",
       "20                D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 6_cell 1_25nM TG_Linear unmixing_0_cmle.ome.tiff\n",
       "21                D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 6_cell 2_25nM TG_Linear unmixing_0_cmle.ome.tiff\n",
       "22                D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 6_cell 3_25nM TG_Linear unmixing_0_cmle.ome.tiff\n",
       "23                D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 6_cell 4_25nM TG_Linear unmixing_0_cmle.ome.tiff\n",
       "24                D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 6_cell 5_25nM TG_Linear unmixing_0_cmle.ome.tiff\n",
       "25             D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 7_cell 1_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff\n",
       "26             D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 7_cell 2_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff\n",
       "27             D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 7_cell 3_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff\n",
       "28             D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 7_cell 4_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff\n",
       "29             D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 7_cell 5_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff\n",
       "30              D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 8_cell 1_untreated_Linear unmixing_0_cmle.ome.tiff\n",
       "31              D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 8_cell 2_untreated_Linear unmixing_0_cmle.ome.tiff\n",
       "32              D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 8_cell 3_untreated_Linear unmixing_0_cmle.ome.tiff\n",
       "33              D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_conditioned_well 8_cell 4_untreated_Linear unmixing_0_cmle.ome.tiff\n",
       "34             D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 10_cell 1_25nM TG_Linear unmixing_0_cmle.ome.tiff\n",
       "35             D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 10_cell 2_25nM TG_Linear unmixing_0_cmle.ome.tiff\n",
       "36             D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 10_cell 3_25nM TG_Linear unmixing_0_cmle.ome.tiff\n",
       "37             D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 10_cell 4_25nM TG_Linear unmixing_0_cmle.ome.tiff\n",
       "38             D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 10_cell 5_25nM TG_Linear unmixing_0_cmle.ome.tiff\n",
       "39          D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 11_cell 1_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff\n",
       "40          D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 11_cell 2_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff\n",
       "41          D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 11_cell 3_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff\n",
       "42          D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 11_cell 4_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff\n",
       "43          D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 11_cell 5_50uM NaAsO_Linear unmixing_0_cmle.ome.tiff\n",
       "44           D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 12_cell 1_untreated_Linear unmixing_0_cmle.ome.tiff\n",
       "45           D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 12_cell 2_untreated_Linear unmixing_0_cmle.ome.tiff\n",
       "46           D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 12_cell 3_untreated_Linear unmixing_0_cmle.ome.tiff\n",
       "47           D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 12_cell 4_untreated_Linear unmixing_0_cmle.ome.tiff\n",
       "48  D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 9_cell 1_50uM NaAsO  washout_Linear unmixing_0_cmle.ome.tiff\n",
       "49  D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 9_cell 2_50uM NaAsO  washout_Linear unmixing_0_cmle.ome.tiff\n",
       "50  D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 9_cell 3_50uM NaAsO  washout_Linear unmixing_0_cmle.ome.tiff\n",
       "51  D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 9_cell 4_50uM NaAsO  washout_Linear unmixing_0_cmle.ome.tiff\n",
       "52  D:\\Experiments (C2-117 - current)\\C2-121\\C2-121_deconvolution\\20230727_C2-121_unconditioned_well 9_cell 5_50uM NaAsO  washout_Linear unmixing_0_cmle.ome.tiff"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_file_list = list_image_files(Path(raw_data),\".tiff\")\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.DataFrame({\"Image Name\":raw_file_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &#x1F6D1; &#x270D; **User Input required:**\n",
    "&#x1F53C; Use the list  above to determine the index of the image you would like to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### USER INPUT REQUIRED ###\n",
    "# Utilizing the list above as reference, change this index number (left column in table) to select a specific image\n",
    "num = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#x1F3C3; **Run code; no user input required**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image name:\n",
      "20230727_C2-121_conditioned_well 2_cell 1_50uM NaAsO_Linear unmixing_0_cmle.ome\n",
      "loaded  inferred 4D `masks_A`  from D:\\Experiments (C2-117 - current)\\C2-121\\20230921_C2-121_3D-analysis\\20230921_C2-121_segmentation \n",
      "loaded  inferred 3D `lyso`  from D:\\Experiments (C2-117 - current)\\C2-121\\20230921_C2-121_3D-analysis\\20230921_C2-121_segmentation \n",
      "loaded  inferred 3D `mito`  from D:\\Experiments (C2-117 - current)\\C2-121\\20230921_C2-121_3D-analysis\\20230921_C2-121_segmentation \n",
      "loaded  inferred 3D `golgi`  from D:\\Experiments (C2-117 - current)\\C2-121\\20230921_C2-121_3D-analysis\\20230921_C2-121_segmentation \n",
      "loaded  inferred 3D `perox`  from D:\\Experiments (C2-117 - current)\\C2-121\\20230921_C2-121_3D-analysis\\20230921_C2-121_segmentation \n",
      "loaded  inferred 3D `ER`  from D:\\Experiments (C2-117 - current)\\C2-121\\20230921_C2-121_3D-analysis\\20230921_C2-121_segmentation \n",
      "loaded  inferred 3D `LD`  from D:\\Experiments (C2-117 - current)\\C2-121\\20230921_C2-121_3D-analysis\\20230921_C2-121_segmentation \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Image layer 'mask_seg' at 0x2831562b580>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_img_data, raw_meta_dict = read_czi_image(raw_file_list[num])\n",
    "print(\"Image name:\")\n",
    "print(raw_meta_dict['name'][0].split(\" :: \")[0])\n",
    "\n",
    "mask_seg = _import_inferred_organelle(mask_suffix, raw_meta_dict, Path(segmentation_data), seg_file_type)\n",
    "lyso_seg = _import_inferred_organelle(lyso_suffix, raw_meta_dict, Path(segmentation_data), seg_file_type)\n",
    "mito_seg = _import_inferred_organelle(mito_suffix, raw_meta_dict, Path(segmentation_data), seg_file_type)\n",
    "golgi_seg = _import_inferred_organelle(golgi_suffix, raw_meta_dict, Path(segmentation_data), seg_file_type)\n",
    "perox_seg = _import_inferred_organelle(perox_suffix, raw_meta_dict, Path(segmentation_data), seg_file_type)\n",
    "ER_seg = _import_inferred_organelle(ER_suffix, raw_meta_dict, Path(segmentation_data), seg_file_type)\n",
    "LD_seg = _import_inferred_organelle(LD_suffix, raw_meta_dict, Path(segmentation_data), seg_file_type)\n",
    "\n",
    "if segmentation_data_2 is not None:\n",
    "    mask_seg = _import_inferred_organelle(mask_suffix, raw_meta_dict, Path(segmentation_data_2), seg_file_type)\n",
    "    lyso_seg = _import_inferred_organelle(lyso_suffix, raw_meta_dict, Path(segmentation_data_2), seg_file_type)\n",
    "    mito_seg = _import_inferred_organelle(mito_suffix, raw_meta_dict, Path(segmentation_data_2), seg_file_type)\n",
    "    golgi_seg = _import_inferred_organelle(golgi_suffix, raw_meta_dict, Path(segmentation_data_2), seg_file_type)\n",
    "    perox_seg = _import_inferred_organelle(perox_suffix, raw_meta_dict, Path(segmentation_data_2), seg_file_type)\n",
    "    ER_seg = _import_inferred_organelle(ER_suffix, raw_meta_dict, Path(segmentation_data_2), seg_file_type)\n",
    "    LD_seg = _import_inferred_organelle(LD_suffix, raw_meta_dict, Path(segmentation_data_2), seg_file_type)\n",
    "\n",
    "viewer.layers.clear()\n",
    "viewer.add_image(raw_img_data[0], name='LD_raw', blending='additive')\n",
    "viewer.add_image(LD_seg, opacity=0.3, colormap='magenta')\n",
    "viewer.add_image(raw_img_data[1], name='ER_raw', blending='additive')\n",
    "viewer.add_image(ER_seg, opacity=0.3, colormap='red')\n",
    "viewer.add_image(raw_img_data[2], name='GL_raw', blending='additive')\n",
    "viewer.add_image(golgi_seg, opacity=0.3, colormap='yellow')\n",
    "viewer.add_image(raw_img_data[3], name='LS_raw', blending='additive')\n",
    "viewer.add_image(lyso_seg, opacity=0.3, colormap='cyan')\n",
    "viewer.add_image(raw_img_data[4], name='MT_raw', blending='additive')\n",
    "viewer.add_image(mito_seg, opacity=0.3, colormap='green')\n",
    "viewer.add_image(raw_img_data[5], name='PO_raw', blending='additive')\n",
    "viewer.add_image(perox_seg, opacity=0.3, colormap='bop orange')\n",
    "viewer.add_image(mask_seg, opacity=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#x1F6D1; **STOP: Use the `Napari` window to review all of the segmentations for this image.** &#x1F50E;\n",
    "\n",
    "#### At this point, take note of which segmentations need to be edited, if any. \n",
    "\n",
    "#### Once you are finished reviewing the images, continue on to the next sections to 1) Edit the segmentation (if necessary) or 2) Save the final set of segmentations for this image in a new folder. This will make preparing for quantitative analysis much simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________\n",
    "## **EDITING SEGMENTATIONS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &#x1F6D1; &#x270D; **User Input:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### USER INPUT REQUIRED ###\n",
    "# Indicate which segmentations need editing by typing True. If the segmentations are good and do not need editing, indicate False.\n",
    "edit_cell = False\n",
    "edit_nuc = False\n",
    "edit_LD = False \n",
    "edit_ER = False\n",
    "edit_golgi = False\n",
    "edit_lyso = False\n",
    "edit_mito = False\n",
    "edit_perox = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#x1F3C3; **Run code; no user input required** \n",
    "### &#x1F440; **See code block output for instructions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head to the Napari window!\n",
      "You can edit your segmentation as needed there.\n",
      "Be sure to save the new segmentation using File > Save in the Napari window. You should save it to the folder you listed as 'location_tosave_edited_segmentations'\n"
     ]
    }
   ],
   "source": [
    "if edit_cell is True:\n",
    "    viewer.layers.clear()\n",
    "    viewer.add_image(raw_img_data)\n",
    "    viewer.add_labels(mask_seg[1])\n",
    "    print(\"Head to the Napari window!\")\n",
    "    print(\"You can edit your segmentation as needed there.\")\n",
    "    print(\"Be sure to save the new segmentation using File > Save in the Napari window. You should save it to the folder you listed as 'location_tosave_edited_segmentations'\")\n",
    "elif edit_cell is False:\n",
    "    print(\"Continue - run the next block of code\")\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue - run the next block of code\n"
     ]
    }
   ],
   "source": [
    "if edit_nuc is True:\n",
    "    viewer.layers.clear()\n",
    "    viewer.add_image(raw_img_data)\n",
    "    viewer.add_labels(mask_seg[2])\n",
    "    print(\"Head to the Napari window!\")\n",
    "    print(\"You can edit your segmentation as needed there.\")\n",
    "    print(\"Be sure to save the new segmentation using File > Save in the Napari window. You should save it to the folder you listed as 'location_tosave_edited_segmentations'\")\n",
    "elif edit_nuc is False:\n",
    "    print(\"Continue - run the next block of code\")\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue - run the next block of code\n"
     ]
    }
   ],
   "source": [
    "if edit_LD is True:\n",
    "    viewer.layers.clear()\n",
    "    viewer.add_image(raw_img_data)\n",
    "    viewer.add_labels(LD_seg)\n",
    "    print(\"Head to the Napari window!\")\n",
    "    print(\"You can edit your segmentation as needed there.\")\n",
    "    print(\"Be sure to save the new segmentation using File > Save in the Napari window. You should save it to the folder you listed as 'location_tosave_edited_segmentations'\")\n",
    "elif edit_LD is False:\n",
    "    print(\"Continue - run the next block of code\")\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue - run the next block of code\n"
     ]
    }
   ],
   "source": [
    "if edit_ER is True:\n",
    "    viewer.layers.clear()\n",
    "    viewer.add_image(raw_img_data)\n",
    "    viewer.add_labels(ER_seg)\n",
    "    print(\"Head to the Napari window!\")\n",
    "    print(\"You can edit your segmentation as needed there.\")\n",
    "    print(\"Be sure to save the new segmentation using File > Save in the Napari window. You should save it to the folder you listed as 'location_tosave_edited_segmentations'\")\n",
    "elif edit_ER is False:\n",
    "    print(\"Continue - run the next block of code\")\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue - run the next block of code\n"
     ]
    }
   ],
   "source": [
    "if edit_golgi is True:\n",
    "    viewer.layers.clear()\n",
    "    viewer.add_image(raw_img_data)\n",
    "    viewer.add_labels(golgi_seg)\n",
    "    print(\"Head to the Napari window!\")\n",
    "    print(\"You can edit your segmentation as needed there.\")\n",
    "    print(\"Be sure to save the new segmentation using File > Save in the Napari window. You should save it to the folder you listed as 'location_tosave_edited_segmentations'\")\n",
    "elif edit_golgi is False:\n",
    "    print(\"Continue - run the next block of code\")\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue - run the next block of code\n"
     ]
    }
   ],
   "source": [
    "if edit_lyso is True:\n",
    "    viewer.layers.clear()\n",
    "    viewer.add_image(raw_img_data)\n",
    "    viewer.add_labels(lyso_seg)\n",
    "    print(\"Head to the Napari window!\")\n",
    "    print(\"You can edit your segmentation as needed there.\")\n",
    "    print(\"Be sure to save the new segmentation using File > Save in the Napari window. You should save it to the folder you listed as 'location_tosave_edited_segmentations'\")\n",
    "elif edit_lyso is False:\n",
    "    print(\"Continue - run the next block of code\")\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue - run the next block of code\n"
     ]
    }
   ],
   "source": [
    "if edit_mito is True:\n",
    "    viewer.layers.clear()\n",
    "    viewer.add_image(raw_img_data)\n",
    "    viewer.add_labels(mito_seg)\n",
    "    print(\"Head to the Napari window!\")\n",
    "    print(\"You can edit your segmentation as needed there.\")\n",
    "    print(\"Be sure to save the new segmentation using File > Save in the Napari window. You should save it to the folder you listed as 'location_tosave_edited_segmentations'\")\n",
    "elif edit_mito is False:\n",
    "    print(\"Continue - run the next block of code\")\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue - run the next block of code\n"
     ]
    }
   ],
   "source": [
    "if edit_perox is True:\n",
    "    viewer.layers.clear()\n",
    "    viewer.add_image(raw_img_data)\n",
    "    viewer.add_labels(perox_seg)\n",
    "    print(\"Head to the Napari window!\")\n",
    "    print(\"You can edit your segmentation as needed there.\")\n",
    "    print(\"Be sure to save the new segmentation using File > Save in the Napari window. You should save it to the folder you listed as 'location_tosave_edited_segmentations'\")\n",
    "elif edit_perox is False:\n",
    "    print(\"Continue - run the next block of code\")\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________\n",
    "# **SAVE ALL CORRECT SEGMENTATIONS** - into one folder for quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved file: 20230727_C2-121_conditioned_well 2_cell 1_50uM NaAsO_Linear unmixing_0_cmle.ome-cell-copy\n"
     ]
    }
   ],
   "source": [
    "if edit_cell is True:\n",
    "    cell_seg = _import_inferred_organelle(\"cell\", raw_meta_dict, location_tosave_edited_segmentations, seg_file_type)\n",
    "    out_file_n = export_inferred_organelle(cell_seg, \"cell\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "elif edit_cell is False:\n",
    "    cell_seg = mask_seg[1]\n",
    "    out_file_n = export_inferred_organelle(cell_seg, \"cell\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head to the Napari window!\n",
      "You can edit your segmentation as needed there.\n",
      "Be sure to save the new segmentation using File > Save in the Napari window. You should save it to the folder you listed as 'location_tosave_edited_segmentations'\n"
     ]
    }
   ],
   "source": [
    "if edit_nuc is True:\n",
    "    nuc_seg = _import_inferred_organelle(\"nuc\", raw_meta_dict, location_tosave_edited_segmentations, seg_file_type)\n",
    "    out_file_n = export_inferred_organelle(cell_seg, \"nuc\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "elif edit_cell is False:\n",
    "    nuc_seg = mask_seg[2]\n",
    "    out_file_n = export_inferred_organelle(cell_seg, \"nuc\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head to the Napari window!\n",
      "You can edit your segmentation as needed there.\n",
      "Be sure to save the new segmentation using File > Save in the Napari window. You should save it to the folder you listed as 'location_tosave_edited_segmentations'\n"
     ]
    }
   ],
   "source": [
    "if edit_LD is True:\n",
    "    LD_seg = _import_inferred_organelle(\"LD\", raw_meta_dict, location_tosave_edited_segmentations, seg_file_type)\n",
    "    out_file_n = export_inferred_organelle(LD_seg, \"LD\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "elif edit_LD is False:\n",
    "    out_file_n = export_inferred_organelle(LD_seg, \"LD\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head to the Napari window!\n",
      "You can edit your segmentation as needed there.\n",
      "Be sure to save the new segmentation using File > Save in the Napari window. You should save it to the folder you listed as 'location_tosave_edited_segmentations'\n"
     ]
    }
   ],
   "source": [
    "if edit_ER is True:\n",
    "    ER_seg = _import_inferred_organelle(\"ER\", raw_meta_dict, location_tosave_edited_segmentations, seg_file_type)\n",
    "    out_file_n = export_inferred_organelle(ER_seg, \"ER\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "elif edit_ER is False:\n",
    "    out_file_n = export_inferred_organelle(ER_seg, \"ER\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head to the Napari window!\n",
      "You can edit your segmentation as needed there.\n",
      "Be sure to save the new segmentation using File > Save in the Napari window. You should save it to the folder you listed as 'location_tosave_edited_segmentations'\n"
     ]
    }
   ],
   "source": [
    "if edit_golgi is True:\n",
    "    golgi_seg = _import_inferred_organelle(\"golgi\", raw_meta_dict, location_tosave_edited_segmentations, seg_file_type)\n",
    "    out_file_n = export_inferred_organelle(golgi_seg, \"golgi\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "elif edit_golgi is False:\n",
    "    out_file_n = export_inferred_organelle(golgi_seg, \"golgi\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head to the Napari window!\n",
      "You can edit your segmentation as needed there.\n",
      "Be sure to save the new segmentation using File > Save in the Napari window. You should save it to the folder you listed as 'location_tosave_edited_segmentations'\n"
     ]
    }
   ],
   "source": [
    "if edit_lyso is True:\n",
    "    lyso_seg = _import_inferred_organelle(\"lyso\", raw_meta_dict, location_tosave_edited_segmentations, seg_file_type)\n",
    "    out_file_n = export_inferred_organelle(lyso_seg, \"lyso\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "elif edit_lyso is False:\n",
    "    out_file_n = export_inferred_organelle(lyso_seg, \"lyso\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head to the Napari window!\n",
      "You can edit your segmentation as needed there.\n",
      "Be sure to save the new segmentation using File > Save in the Napari window. You should save it to the folder you listed as 'location_tosave_edited_segmentations'\n"
     ]
    }
   ],
   "source": [
    "if edit_mito is True:\n",
    "    mito_seg = _import_inferred_organelle(\"mito\", raw_meta_dict, location_tosave_edited_segmentations, seg_file_type)\n",
    "    out_file_n = export_inferred_organelle(mito_seg, \"mito\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "elif edit_mito is False:\n",
    "    out_file_n = export_inferred_organelle(mito_seg, \"mito\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head to the Napari window!\n",
      "You can edit your segmentation as needed there.\n",
      "Be sure to save the new segmentation using File > Save in the Napari window. You should save it to the folder you listed as 'location_tosave_edited_segmentations'\n"
     ]
    }
   ],
   "source": [
    "if edit_perox is True:\n",
    "    perox_seg = _import_inferred_organelle(\"perox\", raw_meta_dict, location_tosave_edited_segmentations, seg_file_type)\n",
    "    out_file_n = export_inferred_organelle(perox_seg, \"perox\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "elif edit_perox is False:\n",
    "    out_file_n = export_inferred_organelle(perox_seg, \"perox\", raw_meta_dict, Path(location_tosave_fullset_gooddata))\n",
    "else:\n",
    "    print(\"There is an error somewhere!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________\n",
    "## ⚠️ **WORK IN PROGRESS:** Quantifying segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_existing_combo(contact, contact_list, splitter):\n",
    "    for ctc in contact_list:\n",
    "        if sorted(contact) == sorted(ctc.split(splitter)):\n",
    "            return(ctc.split(splitter))\n",
    "    return contact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for convex hull errors\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", RuntimeWarning)\n",
    "\n",
    "def _batch_process_quantification(out_file_name: str,\n",
    "                                  seg_path: Union[Path,str],\n",
    "                                  out_path: Union[Path, str], \n",
    "                                  raw_path: Union[Path,str], \n",
    "                                  raw_file_type: str,\n",
    "                                  organelle_names: List[str],\n",
    "                                  organelle_channels: List[int],\n",
    "                                  region_names: List[str],\n",
    "                                  masks_file_name: list[str],\n",
    "                                  mask: str,\n",
    "                                  dist_centering_obj:str, \n",
    "                                  dist_num_bins: int,\n",
    "                                  dist_center_on: bool=False,\n",
    "                                  dist_keep_center_as_bin: bool=True,\n",
    "                                  dist_zernike_degrees: Union[int, None]=None,\n",
    "                                  include_contact_dist: bool = True,\n",
    "                                  scale:bool=True,\n",
    "                                  seg_suffix:Union[str, None]=None,\n",
    "                                  splitter: str = '_') -> int :\n",
    "    \"\"\"  \n",
    "    batch process segmentation quantification (morphology, distribution, contacts); this function is currently optimized to process images from one file folder per image type (e.g., raw, segmentation)\n",
    "    the output csv files are saved to the indicated out_path folder\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    out_file_name: str\n",
    "        the prefix to use when naming the output datatables\n",
    "    seg_path: Union[Path,str]\n",
    "        Path or str to the folder that contains the segmentation tiff files\n",
    "    out_path: Union[Path, str]\n",
    "        Path or str to the folder that the output datatables will be saved to\n",
    "    raw_path: Union[Path,str]\n",
    "        Path or str to the folder that contains the raw image files\n",
    "    raw_file_type: str\n",
    "        the file type of the raw data; ex - \".tiff\", \".czi\"\n",
    "    organelle_names: List[str]\n",
    "        a list of all organelle names that will be analyzed; the names should be the same as the suffix used to name each of the tiff segmentation files\n",
    "        Note: the intensity measurements collect per region (from get_region_morphology_3D function) will only be from channels associated to these organelles \n",
    "    organelle_channels: List[int]\n",
    "        a list of channel indices associated to respective organelle staining in the raw image; the indices should listed in same order in which the respective segmentation name is listed in organelle_names\n",
    "    region_names: List[str]\n",
    "        a list of regions, or masks, to measure; the order should correlate to the order of the channels in the \"masks\" output segmentation file\n",
    "    masks_file_name: str\n",
    "        the suffix of the \"masks\" segmentation file; ex- \"masks_B\", \"masks\", etc.\n",
    "        this function currently does not accept indivial region segmentations \n",
    "    mask: str\n",
    "        the name of the region to use as the mask when measuring the organelles; this should be one of the names listed in regions list; usually this will be the \"cell\" mask\n",
    "    dist_centering_obj:str\n",
    "        the name of the region or object to use as the centering object in the get_XY_distribution function\n",
    "    dist_num_bins: int\n",
    "        the number of bins for the get_XY_distribution function\n",
    "    dist_center_on: bool=False,\n",
    "        for get_XY_distribution:\n",
    "        True = distribute the bins from the center of the centering object\n",
    "        False = distribute the bins from the edge of the centering object\n",
    "    dist_keep_center_as_bin: bool=True\n",
    "        for get_XY_distribution:\n",
    "        True = include the centering object area when creating the bins\n",
    "        False = do not include the centering object area when creating the bins\n",
    "    dist_zernike_degrees: Union[int, None]=None\n",
    "        for get_XY_distribution:\n",
    "        the number of zernike degrees to include for the zernike shape descriptors; if None, the zernike measurements will not \n",
    "        be included in the output\n",
    "    include_contact_dist:bool=True\n",
    "        whether to include the distribution of contact sites in get_contact_metrics_3d(); True = include contact distribution\n",
    "    scale:bool=True\n",
    "        a tuple that contains the real world dimensions for each dimension in the image (Z, Y, X)\n",
    "    seg_suffix:Union[str, None]=None\n",
    "        any additional text that is included in the segmentation tiff files between the file stem and the segmentation suffix\n",
    "        TODO: this can't be None!!! need to update!!!\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    count: int\n",
    "        the number of images processed\n",
    "        \n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    count = 0\n",
    "\n",
    "    if isinstance(raw_path, str): raw_path = Path(raw_path)\n",
    "    if isinstance(seg_path, str): seg_path = Path(seg_path)\n",
    "    if isinstance(out_path, str): out_path = Path(out_path)\n",
    "    \n",
    "    if not Path.exists(out_path):\n",
    "        Path.mkdir(out_path)\n",
    "        print(f\"making {out_path}\")\n",
    "    \n",
    "    # reading list of files from the raw path\n",
    "    img_file_list = list_image_files(raw_path, raw_file_type)\n",
    "\n",
    "    # list of segmentation files to collect\n",
    "    segs_to_collect = organelle_names + masks_file_name\n",
    "\n",
    "    # containers to collect data tabels\n",
    "    org_tabs = []\n",
    "    contact_tabs = []\n",
    "    dist_tabs = []\n",
    "    region_tabs = []\n",
    "    for img_f in img_file_list:\n",
    "        count = count + 1\n",
    "        filez = find_segmentation_tiff_files(img_f, segs_to_collect, seg_path, seg_suffix)\n",
    "\n",
    "        # read in raw file and metadata\n",
    "        img_data, meta_dict = read_czi_image(filez[\"raw\"])\n",
    "\n",
    "        # create intensities from raw file as list based on the channel order provided\n",
    "        intensities = [img_data[ch] for ch in organelle_channels]\n",
    "\n",
    "        # define the scale\n",
    "        if scale is True:\n",
    "            scale_tup = meta_dict['scale']\n",
    "        else:\n",
    "            scale_tup = None\n",
    "\n",
    "        # load regions as a list based on order in list (should match order in \"masks\" file)\n",
    "        # masks = read_tiff_image(filez[masks_file_name]) \n",
    "        # regions = [masks[r] for r, region in enumerate(region_names)]\n",
    "        regions= [read_tiff_image(filez[masks_file_name[0]]), read_tiff_image(filez[masks_file_name[1]])]\n",
    "\n",
    "        # store organelle images as list\n",
    "        organelles = [read_tiff_image(filez[org]) for org in organelle_names]\n",
    "\n",
    "        org_metrics, contact_metrics, dist_metrics, region_metrics = make_all_metrics_tables(source_file=img_f,\n",
    "                                                                                             list_obj_names=organelle_names,\n",
    "                                                                                             list_obj_segs=organelles,\n",
    "                                                                                             list_intensity_img=intensities, \n",
    "                                                                                             list_region_names=region_names,\n",
    "                                                                                             list_region_segs=regions, \n",
    "                                                                                             mask=mask,\n",
    "                                                                                             dist_centering_obj=dist_centering_obj,\n",
    "                                                                                             dist_num_bins=dist_num_bins,\n",
    "                                                                                             dist_center_on=dist_center_on,\n",
    "                                                                                             dist_keep_center_as_bin=dist_keep_center_as_bin,\n",
    "                                                                                             dist_zernike_degrees=dist_zernike_degrees,\n",
    "                                                                                             scale=scale_tup,\n",
    "                                                                                             include_contact_dist=include_contact_dist,\n",
    "                                                                                             splitter=splitter)\n",
    "\n",
    "        org_tabs.append(org_metrics)\n",
    "        contact_tabs.append(contact_metrics)\n",
    "        dist_tabs.append(dist_metrics)\n",
    "        region_tabs.append(region_metrics)\n",
    "        end2 = time.time()\n",
    "        print(f\"Completed processing for {count} images in {(end2-start)/60} mins.\")\n",
    "\n",
    "    final_org = pd.concat(org_tabs, ignore_index=True)\n",
    "    final_contact = pd.concat(contact_tabs, ignore_index=True)\n",
    "    final_dist = pd.concat(dist_tabs, ignore_index=True)\n",
    "    final_region = pd.concat(region_tabs, ignore_index=True)\n",
    "\n",
    "    org_csv_path = out_path / f\"{out_file_name}organelles.csv\"\n",
    "    final_org.to_csv(org_csv_path)\n",
    "\n",
    "    contact_csv_path = out_path / f\"{out_file_name}contacts.csv\"\n",
    "    final_contact.to_csv(contact_csv_path)\n",
    "\n",
    "    dist_csv_path = out_path / f\"{out_file_name}distributions.csv\"\n",
    "    final_dist.to_csv(dist_csv_path)\n",
    "\n",
    "    region_csv_path = out_path / f\"{out_file_name}regions.csv\"\n",
    "    final_region.to_csv(region_csv_path)\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"Quantification for {count} files is COMPLETE! Files saved to '{out_path}'.\")\n",
    "    print(f\"It took {(end - start)/60} minutes to quantify these files.\")\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "It took 11.197751875718435 minutes to quantify one image.\n",
      "Completed processing for 1 images in 11.630062663555146 mins.\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "It took 16.24935408035914 minutes to quantify one image.\n",
      "Completed processing for 2 images in 28.92150789499283 mins.\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "It took 17.200320132573445 minutes to quantify one image.\n",
      "Completed processing for 3 images in 47.09849148988724 mins.\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "It took 14.796633998552958 minutes to quantify one image.\n",
      "Completed processing for 4 images in 62.76304972569148 mins.\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "It took 12.431951602300009 minutes to quantify one image.\n",
      "Completed processing for 5 images in 76.18040299812952 mins.\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "It took 11.224638803799946 minutes to quantify one image.\n",
      "Completed processing for 6 images in 88.2207192103068 mins.\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "It took 14.207482234636943 minutes to quantify one image.\n",
      "Completed processing for 7 images in 103.56214294433593 mins.\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "It took 8.807079414526621 minutes to quantify one image.\n",
      "Completed processing for 8 images in 113.19148420890173 mins.\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "It took 8.355920660495759 minutes to quantify one image.\n",
      "Completed processing for 9 images in 122.26812825202941 mins.\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "It took 15.71372929016749 minutes to quantify one image.\n",
      "Completed processing for 10 images in 138.91479587157568 mins.\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "It took 17.84711666504542 minutes to quantify one image.\n",
      "Completed processing for 11 images in 157.70761824448903 mins.\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "It took 9.764706953366597 minutes to quantify one image.\n",
      "Completed processing for 12 images in 168.2663462638855 mins.\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "It took 23.11478656133016 minutes to quantify one image.\n",
      "Completed processing for 13 images in 192.6603833556175 mins.\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "It took 16.509058141708373 minutes to quantify one image.\n",
      "Completed processing for 14 images in 210.33981264829634 mins.\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "WTF!!  how did we have missing labels?\n",
      "It took 13.264725800355276 minutes to quantify one image.\n",
      "Completed processing for 15 images in 224.39484720230104 mins.\n",
      "Quantification for 15 files is COMPLETE! Files saved to 'D:\\Experiments (C2-117 - current)\\C2-123\\20230922_C2-123_3D-analysis\\20231117_prelim_quant'.\n",
      "It took 224.56879546642304 minutes to quantify these files.\n"
     ]
    }
   ],
   "source": [
    "seg=_batch_process_quantification(out_file_name= \"20231117_prelim\",\n",
    "                                  seg_path=\"C:/Users/zscoman/Documents/Python Scripts/Infer-subc-2D/out\",\n",
    "                                  out_path=\"C:/Users/zscoman/Documents/Python Scripts/Infer-subc-2D/data/test\", \n",
    "                                  raw_path=\"C:/Users/zscoman/Documents/Python Scripts/Infer-subc-2D/raw/shannon\",\n",
    "                                  raw_file_type = \".tiff\",\n",
    "                                  organelle_names = ['LD', 'ER', 'golgi', 'lyso', 'mito', 'perox'],\n",
    "                                  organelle_channels= [0,1,2,3,4,5],\n",
    "                                  region_names= ['nuc', 'cell'],\n",
    "                                  masks_file_name= ['nuc', 'cell'],\n",
    "                                  mask= 'cell',\n",
    "                                  dist_centering_obj='nuc', \n",
    "                                  dist_num_bins=5,\n",
    "                                  dist_center_on=False,\n",
    "                                  dist_keep_center_as_bin=True,\n",
    "                                  dist_zernike_degrees=None,\n",
    "                                  include_contact_dist= True,\n",
    "                                  scale=True,\n",
    "                                  seg_suffix=\"-\",\n",
    "                                  splitter='X')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________\n",
    "## Collecting Summary Stats across multiple experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _batch_summary_stats(csv_path_list: List[str],\n",
    "                         out_path: str,\n",
    "                         out_preffix: str,\n",
    "                         splitter: str='X'):\n",
    "    \"\"\"\" \n",
    "    csv_path_list: List[str],\n",
    "        A list of path strings where .csv files to analyze are located.\n",
    "    out_path: str,\n",
    "        A path string where the summary data file will be output to\n",
    "    out_preffix: str\n",
    "        The prefix used to name the output file.    \n",
    "    \"\"\"\n",
    "    ds_count = 0\n",
    "    fl_count = 0\n",
    "    ###################\n",
    "    # Read in the csv files and combine them into one of each type\n",
    "    ###################\n",
    "    org_tabs = []\n",
    "    contact_tabs = []\n",
    "    dist_tabs = []\n",
    "    region_tabs = []\n",
    "\n",
    "    for loc in csv_path_list:\n",
    "        print(loc)\n",
    "        ds_count = ds_count + 1\n",
    "        loc=Path(loc)\n",
    "        files_store = sorted(loc.glob(\"*.csv\"))\n",
    "        for file in files_store:\n",
    "            fl_count = fl_count + 1\n",
    "            stem = file.stem\n",
    "\n",
    "            org = \"organelles\"\n",
    "            contacts = \"contacts\"\n",
    "            dist = \"distributions\"\n",
    "            regions = \"regions\"\n",
    "\n",
    "            if org in stem:\n",
    "                test_orgs = pd.read_csv(file, index_col=0)\n",
    "                test_orgs.insert(0, \"dataset\", stem[:-11])\n",
    "                org_tabs.append(test_orgs)\n",
    "            if contacts in stem:\n",
    "                test_contact = pd.read_csv(file, index_col=0)\n",
    "                test_contact.insert(0, \"dataset\", stem[:-9])\n",
    "                contact_tabs.append(test_contact)\n",
    "            if dist in stem:\n",
    "                test_dist = pd.read_csv(file, index_col=0)\n",
    "                test_dist.insert(0, \"dataset\", stem[:-14])\n",
    "                dist_tabs.append(test_dist)\n",
    "            if regions in stem:\n",
    "                test_regions = pd.read_csv(file, index_col=0)\n",
    "                test_regions.insert(0, \"dataset\", stem[:-8])\n",
    "                region_tabs.append(test_regions)\n",
    "            \n",
    "    org_df = pd.concat(org_tabs,axis=0, join='outer')\n",
    "    contacts_df = pd.concat(contact_tabs,axis=0, join='outer')\n",
    "    dist_df = pd.concat(dist_tabs,axis=0, join='outer')\n",
    "    regions_df = pd.concat(region_tabs,axis=0, join='outer')\n",
    "    ##########################\n",
    "    # List organelles in cell\n",
    "    ###########################\n",
    "    all_orgs = list(set(org_df.loc[:, 'object'].tolist()))\n",
    "\n",
    "    ###################\n",
    "    # adding new metrics to the original sheets\n",
    "    ###################\n",
    "    # TODO: include these labels when creating the original sheets\n",
    "    contact_cnt = contacts_df[[\"dataset\", \"image_name\", \"object\", \"label\", \"volume\"]]\n",
    "    ctc = contact_cnt[\"object\"].values.tolist()\n",
    "    ##############################################################################\n",
    "    #  Creating New methods of storing A & B\n",
    "    ###############################################################################\n",
    "    # len(max(contact_cnt[\"object\"].str.split('X'), key=len))) provides max number of organelles involved in contact\n",
    "    contact_cnt[[f\"org{cha}\" for cha in string.ascii_uppercase[:(len(max(contact_cnt[\"object\"].str.split(splitter), key=len)))]]] = contact_cnt[\"object\"].str.split(splitter, expand=True)\n",
    "    contact_cnt[[f\"{cha}_ID\" for cha in string.ascii_uppercase[:(len(max(contact_cnt[\"label\"].str.split('_'), key=len)))]]] = contact_cnt[\"label\"].str.split('_', expand=True)\n",
    "    #iterating from a to val\n",
    "    unstacked_cont = []\n",
    "    for cha in string.ascii_uppercase[:len(max(contact_cnt[\"object\"].str.split(splitter), key=len))]:\n",
    "        valid = (contact_cnt[f\"org{cha}\"] != None) & (contact_cnt[f\"{cha}_ID\"] != None)\n",
    "        contact_cnt[f\"{cha}\"] = None\n",
    "        contact_cnt.loc[valid, f\"{cha}\"] = contact_cnt[f\"org{cha}\"] + \"_\" + contact_cnt[f\"{cha}_ID\"]\n",
    "        contact_cnt_percell = contact_cnt[[\"dataset\", \"image_name\", f\"org{cha}\", f\"{cha}_ID\", \"object\", \"volume\"]].groupby([\"dataset\", \"image_name\", f\"org{cha}\", f\"{cha}_ID\", \"object\"]).agg([\"count\", \"sum\"])\n",
    "        contact_cnt_percell.columns = [\"_\".join(col_name).rstrip('_') for col_name in contact_cnt_percell.columns.to_flat_index()]\n",
    "        unstacked = contact_cnt_percell.unstack(level='object')\n",
    "        unstacked.columns = [\"_\".join(col_name).rstrip('_') for col_name in unstacked.columns.to_flat_index()]\n",
    "        unstacked = unstacked.reset_index()\n",
    "        for col in unstacked.columns:\n",
    "            if col.startswith(\"volume_count_\"):\n",
    "                newname = col.split(\"_\")[-1] + \"_count\"\n",
    "                unstacked.rename(columns={col:newname}, inplace=True)\n",
    "            if col.startswith(\"volume_sum_\"):\n",
    "                newname = col.split(\"_\")[-1] + \"_volume\"\n",
    "                unstacked.rename(columns={col:newname}, inplace=True)\n",
    "        unstacked.rename(columns={f\"org{cha}\":\"object\", f\"{cha}_ID\":\"label\"}, inplace=True)\n",
    "        unstacked.set_index(['dataset', 'image_name', 'object', 'label'])    \n",
    "        unstacked_cont.append(unstacked)\n",
    "    contact_cnt = pd.concat(unstacked_cont, axis=0).sort_index(axis=0)\n",
    "    contact_cnt = contact_cnt.groupby(['dataset', 'image_name', 'object', 'label']).sum().reset_index()                 #adds together all duplicates at the index, then resets the index\n",
    "    contact_cnt['label']=contact_cnt['label'].astype(\"Int64\")  \n",
    "    org_df = pd.merge(org_df, contact_cnt, how='left', on=['dataset', 'image_name', 'object', 'label'], sort=True)\n",
    "    org_df[contact_cnt.columns] = org_df[contact_cnt.columns].fillna(0)\n",
    "\n",
    "    ###################\n",
    "    # summary stat group\n",
    "    ###################\n",
    "    group_by = ['dataset', 'image_name', 'object']\n",
    "    sharedcolumns = [\"SA_to_volume_ratio\", \"equivalent_diameter\", \"extent\", \"euler_number\", \"solidity\", \"axis_major_length\"]\n",
    "    ag_func_standard = ['mean', 'median', 'std']\n",
    "\n",
    "    ###################\n",
    "    # summarize shared measurements between org_df and contacts_df\n",
    "    ###################\n",
    "    org_cont_tabs = []\n",
    "    for tab in [org_df, contacts_df]:\n",
    "        tab1 = tab[group_by + ['volume']].groupby(group_by).agg(['count', 'sum'] + ag_func_standard)\n",
    "        tab2 = tab[group_by + ['surface_area']].groupby(group_by).agg(['sum'] + ag_func_standard)\n",
    "        tab3 = tab[group_by + sharedcolumns].groupby(group_by).agg(ag_func_standard)\n",
    "        shared_metrics = pd.merge(tab1, tab2, 'outer', on=group_by)\n",
    "        shared_metrics = pd.merge(shared_metrics, tab3, 'outer', on=group_by)\n",
    "        org_cont_tabs.append(shared_metrics)\n",
    "\n",
    "    org_summary = org_cont_tabs[0]\n",
    "    contact_summary = org_cont_tabs[1]\n",
    "\n",
    "    ###################\n",
    "    # group metrics from regions_df similar to the above\n",
    "    ###################\n",
    "    regions_summary = regions_df[group_by + ['volume', 'surface_area'] + sharedcolumns].set_index(group_by)\n",
    "\n",
    "    ###################\n",
    "    # summarize extra metrics from org_df\n",
    "    ###################\n",
    "    columns2 = [col for col in org_df.columns if col.endswith((\"_count\", \"_volume\"))]\n",
    "    contact_counts_summary = org_df[group_by + columns2].groupby(group_by).agg(['sum'] + ag_func_standard)\n",
    "    org_summary = pd.merge(org_summary, contact_counts_summary, 'outer', on=group_by)#left_on=group_by, right_on=True)\n",
    "\n",
    "    ###################\n",
    "    # summarize distribution measurements\n",
    "    ###################\n",
    "    # organelle distributions\n",
    "    hist_dfs = []\n",
    "    for ind in dist_df.index:\n",
    "        selection = dist_df.loc[[ind]]\n",
    "        bins_df = pd.DataFrame()\n",
    "        wedges_df = pd.DataFrame()\n",
    "        Z_df = pd.DataFrame()\n",
    "\n",
    "        bins_df[['bins', 'masks', 'obj']] = selection[['XY_bins', 'XY_mask_vox_cnt_perbin', 'XY_obj_vox_cnt_perbin']]\n",
    "        wedges_df[['bins', 'masks', 'obj']] = selection[['XY_wedges', 'XY_mask_vox_cnt_perwedge', 'XY_obj_vox_cnt_perwedge']]\n",
    "        Z_df[['bins', 'masks', 'obj']] = selection[['Z_slices', 'Z_mask_vox_cnt', 'Z_obj_vox_cnt']]\n",
    "\n",
    "        dfs = [selection[['dataset', 'image_name', 'object']].reset_index()]\n",
    "        for df, prefix in zip([bins_df, wedges_df, Z_df], [\"XY_bins_\", \"XY_wedges_\", \"Z_slices_\"]):\n",
    "            single_df = pd.DataFrame(list(zip(df[\"bins\"].values[0][1:-1].split(\", \"), \n",
    "                                            df[\"obj\"].values[0][1:-1].split(\", \"), \n",
    "                                            df[\"masks\"].values[0][1:-1].split(\", \"))), columns =['bins', 'obj', 'mask']).astype(int)\n",
    "\n",
    "            if \"Z_\" in prefix:\n",
    "                single_df =  single_df.drop(single_df[single_df['mask'] == 0].index)\n",
    "                single_df['bins'] = (single_df[\"bins\"]/max(single_df.bins)*10).apply(np.floor)\n",
    "        \n",
    "            single_df['mask_fract'] = single_df['mask']/single_df['mask'].max()\n",
    "            single_df['obj_norm'] = (single_df[\"obj\"]/single_df[\"mask_fract\"]).fillna(0)\n",
    "            single_df['portion_per_bin'] = (single_df[\"obj\"] / single_df[\"obj\"].sum())*100\n",
    "\n",
    "            # single_df['mask_fract'] = single_df['mask']/single_df['mask'].max()\n",
    "            # single_df['obj_norm'] = (single_df[\"obj\"]/single_df[\"mask_fract\"]).fillna(0)\n",
    "            # single_df['portion_per_bin'] = (single_df[\"obj\"] / single_df[\"obj\"].sum())*100\n",
    "\n",
    "            # if \"Z_\" in prefix:\n",
    "            #     single_df['bins'] = (single_df[\"bins\"]/max(single_df.bins)*10).apply(np.floor)\n",
    "\n",
    "\n",
    "            sumstats_df = pd.DataFrame()\n",
    "\n",
    "            s = single_df['bins'].repeat(single_df['obj_norm'])\n",
    "            sumstats_df['hist_mean']=[s.mean()]\n",
    "            sumstats_df['hist_median']=[s.median()]\n",
    "            if single_df['obj_norm'].sum() != 0: sumstats_df['hist_mode']=[s.mode()[0]]\n",
    "            else: sumstats_df['hist_mode']=['NaN']\n",
    "            sumstats_df['hist_min']=[s.min()]\n",
    "            sumstats_df['hist_max']=[s.max()]\n",
    "            sumstats_df['hist_range']=[s.max() - s.min()]\n",
    "            sumstats_df['hist_stdev']=[s.std()]\n",
    "            sumstats_df['hist_skew']=[s.skew()]\n",
    "            sumstats_df['hist_kurtosis']=[s.kurtosis()]\n",
    "            sumstats_df['hist_var']=[s.var()]\n",
    "            sumstats_df.columns = [prefix+col for col in sumstats_df.columns]\n",
    "            dfs.append(sumstats_df.reset_index())\n",
    "        combined_df = pd.concat(dfs, axis=1).drop(columns=\"index\")\n",
    "        hist_dfs.append(combined_df)\n",
    "    dist_org_summary = pd.concat(hist_dfs, ignore_index=True)\n",
    "\n",
    "    # nucleus distribution\n",
    "    nuc_dist_df = dist_df[[\"dataset\", \"image_name\", \n",
    "                        \"XY_bins\", \"XY_center_vox_cnt_perbin\", \"XY_mask_vox_cnt_perbin\",\n",
    "                        \"XY_wedges\", \"XY_center_vox_cnt_perwedge\", \"XY_mask_vox_cnt_perwedge\",\n",
    "                        \"Z_slices\", \"Z_center_vox_cnt\", \"Z_mask_vox_cnt\"]].set_index([\"dataset\", \"image_name\"])\n",
    "    nuc_hist_dfs = []\n",
    "    for idx in nuc_dist_df.index.unique():\n",
    "        selection = nuc_dist_df.loc[idx].iloc[[0]].reset_index()\n",
    "        bins_df = pd.DataFrame()\n",
    "        wedges_df = pd.DataFrame()\n",
    "        Z_df = pd.DataFrame()\n",
    "\n",
    "        bins_df[['bins', 'center', 'masks']] = selection[['XY_bins', 'XY_center_vox_cnt_perbin', 'XY_mask_vox_cnt_perbin']]\n",
    "        wedges_df[['bins', 'center', 'masks']] = selection[['XY_wedges', 'XY_center_vox_cnt_perwedge', 'XY_mask_vox_cnt_perwedge']]\n",
    "        Z_df[['bins', 'center', 'masks']] = selection[['Z_slices', 'Z_center_vox_cnt', 'Z_mask_vox_cnt']]\n",
    "\n",
    "        dfs = [selection[['dataset', 'image_name']]]\n",
    "        for df, prefix in zip([bins_df, wedges_df, Z_df], [\"XY_bins_\", \"XY_wedges_\", \"Z_slices_\"]):\n",
    "            single_df = pd.DataFrame(list(zip(df[\"bins\"].values[0][1:-1].split(\", \"), \n",
    "                                            df[\"masks\"].values[0][1:-1].split(\", \"),\n",
    "                                            df[\"center\"].values[0][1:-1].split(\", \"))), columns =['bins', 'mask', 'obj']).astype(int)\n",
    "            # single_df['mask_fract'] = single_df['mask']/single_df['mask'].max()\n",
    "            # single_df['obj_norm'] = (single_df[\"obj\"]/single_df[\"mask_fract\"]).fillna(0)\n",
    "            # single_df['portion_per_bin'] = (single_df[\"obj\"] / single_df[\"obj\"].sum())*100\n",
    "            # if \"Z_\" in prefix:\n",
    "            #     single_df['bins'] = (single_df[\"bins\"]/max(single_df.bins)*10).apply(np.floor)\n",
    "\n",
    "            if \"Z_\" in prefix:\n",
    "                single_df =  single_df.drop(single_df[single_df['mask'] == 0].index)\n",
    "                single_df['bins'] = (single_df[\"bins\"]/max(single_df.bins)*10).apply(np.floor)\n",
    "        \n",
    "            single_df['mask_fract'] = single_df['mask']/single_df['mask'].max()\n",
    "            single_df['obj_norm'] = (single_df[\"obj\"]/single_df[\"mask_fract\"]).fillna(0)\n",
    "            single_df['portion_per_bin'] = (single_df[\"obj\"] / single_df[\"obj\"].sum())*100\n",
    "\n",
    "            sumstats_df = pd.DataFrame()\n",
    "\n",
    "            s = single_df['bins'].repeat(single_df['obj_norm'])\n",
    "            sumstats_df['hist_mean']=[s.mean()]\n",
    "            sumstats_df['hist_median']=[s.median()]\n",
    "            if single_df['obj_norm'].sum() != 0: sumstats_df['hist_mode']=[s.mode()[0]]\n",
    "            else: sumstats_df['hist_mode']=['NaN']\n",
    "            sumstats_df['hist_min']=[s.min()]\n",
    "            sumstats_df['hist_max']=[s.max()]\n",
    "            sumstats_df['hist_range']=[s.max() - s.min()]\n",
    "            sumstats_df['hist_stdev']=[s.std()]\n",
    "            sumstats_df['hist_skew']=[s.skew()]\n",
    "            sumstats_df['hist_kurtosis']=[s.kurtosis()]\n",
    "            sumstats_df['hist_var']=[s.var()]\n",
    "            sumstats_df.columns = [prefix+col for col in sumstats_df.columns]\n",
    "            dfs.append(sumstats_df.reset_index())\n",
    "        combined_df = pd.concat(dfs, axis=1).drop(columns=\"index\")\n",
    "        nuc_hist_dfs.append(combined_df)\n",
    "    dist_center_summary = pd.concat(nuc_hist_dfs, ignore_index=True)\n",
    "    dist_center_summary.insert(2, column=\"object\", value=\"nuc\")\n",
    "\n",
    "    dist_summary = pd.concat([dist_org_summary, dist_center_summary], axis=0).set_index(group_by).sort_index()\n",
    "\n",
    "    ###################\n",
    "    # add normalization\n",
    "    ###################\n",
    "    # organelle area fraction\n",
    "    area_fractions = []\n",
    "    for idx in org_summary.index.unique():\n",
    "        org_vol = org_summary.loc[idx][('volume', 'sum')]\n",
    "        cell_vol = regions_summary.loc[idx[:-1] + ('cell',)][\"volume\"]\n",
    "        afrac = org_vol/cell_vol\n",
    "        area_fractions.append(afrac)\n",
    "    org_summary[('volume', 'fraction')] = area_fractions\n",
    "    # TODO: add in line to reorder the level=0 columns here\n",
    "\n",
    "    # contact sites volume normalized\n",
    "    # norm_toA_list = []\n",
    "    # norm_toB_list = []\n",
    "    norm_to_list = {}\n",
    "    for col in contact_summary.index:\n",
    "        for idx, cha in enumerate(string.ascii_uppercase[:len(max(contact_summary.index.get_level_values('object').str.split(splitter), key=len))]):\n",
    "            if cha not in norm_to_list:\n",
    "                norm_to_list[f\"{cha}\"] = []\n",
    "            if ((idx+1) <= len(col[-1].split(splitter))):\n",
    "                norm_to_list[f\"{cha}\"].append(contact_summary.loc[col][('volume', 'sum')]/org_summary.loc[col[:-1]+(col[-1].split(splitter)[idx],)][('volume', 'sum')])\n",
    "            else:\n",
    "                norm_to_list[f\"{cha}\"].append(None)\n",
    "    for cha in string.ascii_uppercase[:len(max(contact_summary.index.get_level_values('object').str.split(splitter), key=len))]:\n",
    "        contact_summary[('volume', f'norm_to_{cha}')] = norm_to_list[f\"{cha}\"]\n",
    "        # norm_toA_list.append(contact_summary.loc[col][('volume', 'sum')]/org_summary.loc[col[:-1]+(col[-1].split(splitter)[0],)][('volume', 'sum')])\n",
    "        # norm_toB_list.append(contact_summary.loc[col][('volume', 'sum')]/org_summary.loc[col[:-1]+(col[-1].split(splitter)[1],)][('volume', 'sum')])\n",
    "    # contact_summary[('volume', 'norm_to_A')] = norm_toA_list\n",
    "    # contact_summary[('volume', 'norm_to_B')] = norm_toB_list\n",
    "\n",
    "    # number and area of individuals organelle involved in contact\n",
    "    cont_cnt = org_df[group_by]\n",
    "    cont_cnt[[col.split('_')[0] for col in org_df.columns if col.endswith((\"_count\"))]] = org_df[[col for col in org_df.columns if col.endswith((\"_count\"))]].astype(bool)\n",
    "    cont_cnt_perorg = cont_cnt.groupby(group_by).agg('sum')\n",
    "    cont_cnt_perorg.columns = pd.MultiIndex.from_product([cont_cnt_perorg.columns, ['count_in']])\n",
    "    for col in cont_cnt_perorg.columns:\n",
    "        cont_cnt_perorg[(col[0], 'num_fraction_in')] = cont_cnt_perorg[col].values/org_summary[('volume', 'count')].values\n",
    "    cont_cnt_perorg.sort_index(axis=1, inplace=True)\n",
    "    org_summary = pd.merge(org_summary, cont_cnt_perorg, on=group_by, how='outer')\n",
    "\n",
    "\n",
    "    ###################\n",
    "    # flatten datasheets and combine\n",
    "    # TODO: restructure this so that all of the datasheets and unstacked and then reorded based on shared level 0 columns before flattening\n",
    "    ###################\n",
    "    # org flattening\n",
    "    org_final = org_summary.unstack(-1)\n",
    "    for col in org_final.columns:\n",
    "        if col[1] in ('count_in', 'num_fraction_in') or col[0].endswith(('_count', '_volume')):\n",
    "            if col[2] not in col[0]:\n",
    "                org_final.drop(col,axis=1, inplace=True)\n",
    "    ########################################################################\n",
    "    # MAKING new_col_order flexible to work with any organelle input values and combo number\n",
    "    #######################################################################\n",
    "    new_col_order = ['dataset', 'image_name', 'object', 'volume', 'surface_area', 'SA_to_volume_ratio', \n",
    "                     'equivalent_diameter', 'extent', 'euler_number', 'solidity', 'axis_major_length'] \n",
    "    all_combos = []\n",
    "    for n in list(map(lambda x:x+2, (range(len(all_orgs)-1)))):\n",
    "            for o in itertools.combinations(all_orgs, n):\n",
    "                all_combos.append(check_for_existing_combo(o, ctc, splitter))\n",
    "    combos = [splitter.join(cont) for cont in all_combos]\n",
    "    for combo in combos:\n",
    "        new_col_order += [f\"{combo}\", f\"{combo}_count\", f\"{combo}_volume\"]\n",
    "    new_cols = org_final.columns.reindex(new_col_order, level=0)\n",
    "    org_final = org_final.reindex(columns=new_cols[0])\n",
    "    org_final.columns = [\"_\".join((col_name[-1], col_name[1], col_name[0])) for col_name in org_final.columns.to_flat_index()]\n",
    "\n",
    "    #renaming, filling \"NaN\" with 0 when needed, and removing ER_std columns\n",
    "    for col in org_final.columns:\n",
    "        if '_count_in_' or '_fraction_in_' in col:\n",
    "            org_final[col] = org_final[col].fillna(0)\n",
    "        if col.endswith((\"_count_volume\",\"_sum_volume\", \"_mean_volume\", \"_median_volume\")):\n",
    "            org_final[col] = org_final[col].fillna(0)\n",
    "        if col.endswith(\"_count_volume\"):\n",
    "            org_final.rename(columns={col:col.split(\"_\")[0]+\"_count\"}, inplace=True)\n",
    "        if col.startswith(\"ER_std_\"):\n",
    "            org_final.drop(columns=[col], inplace=True)\n",
    "    org_final = org_final.reset_index()\n",
    "\n",
    "    # contacts flattened\n",
    "    contact_final = contact_summary.unstack(-1)\n",
    "    contact_final.columns = [\"_\".join((col_name[-1], col_name[1], col_name[0])) for col_name in contact_final.columns.to_flat_index()]\n",
    "\n",
    "    #renaming and filling \"NaN\" with 0 when needed\n",
    "    for col in contact_final.columns:\n",
    "        if col.endswith((\"_count_volume\",\"_sum_volume\", \"_mean_volume\", \"_median_volume\")):\n",
    "            contact_final[col] = contact_final[col].fillna(0)\n",
    "        if col.endswith(\"_count_volume\"):\n",
    "            contact_final.rename(columns={col:col.split(\"_\")[0]+\"_count\"}, inplace=True)\n",
    "    contact_final = contact_final.reset_index()\n",
    "\n",
    "    # distributions flattened\n",
    "    dist_final = dist_summary.unstack(-1)\n",
    "    dist_final.columns = [\"_\".join((col_name[1], col_name[0])) for col_name in dist_final.columns.to_flat_index()]\n",
    "    dist_final = dist_final.reset_index()\n",
    "\n",
    "    # regions flattened & normalization added\n",
    "    regions_final = regions_summary.unstack(-1)\n",
    "    regions_final.columns = [\"_\".join((col_name[1], col_name[0])) for col_name in regions_final.columns.to_flat_index()]\n",
    "    regions_final['nuc_area_fraction'] = regions_final['nuc_volume'] / regions_final['cell_volume']\n",
    "    regions_final = regions_final.reset_index()\n",
    "\n",
    "    # combining them all\n",
    "    combined = pd.merge(org_final, contact_final, on=[\"dataset\", \"image_name\"], how=\"outer\")\n",
    "    combined = pd.merge(combined, dist_final, on=[\"dataset\", \"image_name\"], how=\"outer\")\n",
    "    combined = pd.merge(combined, regions_final, on=[\"dataset\", \"image_name\"], how=\"outer\").set_index([\"dataset\", \"image_name\"])\n",
    "    combined.columns = [col.replace('sum', 'total') for col in combined.columns]\n",
    "\n",
    "    ###################\n",
    "    # export summary sheets\n",
    "    ###################\n",
    "    org_summary.to_csv(out_path + f\"/{out_preffix}per_org_summarystats.csv\")\n",
    "    contact_summary.to_csv(out_path + f\"/{out_preffix}per_contact_summarystats.csv\")\n",
    "    dist_summary.to_csv(out_path + f\"/{out_preffix}distribution_summarystats.csv\")\n",
    "    regions_summary.to_csv(out_path + f\"/{out_preffix}per_region_summarystats.csv\")\n",
    "    combined.to_csv(out_path + f\"/{out_preffix}summarystats_combined.csv\")\n",
    "\n",
    "    print(f\"Processing of {fl_count} files from {ds_count} dataset(s) is complete.\")\n",
    "    return f\"{fl_count} files from {ds_count} dataset(s) were processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shannon\\AppData\\Local\\Temp\\ipykernel_23384\\2055995018.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  contact_cnt[[\"orgA\", \"orgB\"]] = contact_cnt[\"object\"].str.split('X', expand=True)\n",
      "C:\\Users\\Shannon\\AppData\\Local\\Temp\\ipykernel_23384\\2055995018.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  contact_cnt[[\"orgA\", \"orgB\"]] = contact_cnt[\"object\"].str.split('X', expand=True)\n",
      "C:\\Users\\Shannon\\AppData\\Local\\Temp\\ipykernel_23384\\2055995018.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  contact_cnt[[\"A_ID\", \"B_ID\"]] = contact_cnt[\"label\"].str.split('_', expand=True)\n",
      "C:\\Users\\Shannon\\AppData\\Local\\Temp\\ipykernel_23384\\2055995018.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  contact_cnt[[\"A_ID\", \"B_ID\"]] = contact_cnt[\"label\"].str.split('_', expand=True)\n",
      "C:\\Users\\Shannon\\AppData\\Local\\Temp\\ipykernel_23384\\2055995018.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  contact_cnt[\"A\"] = contact_cnt[\"orgA\"] +\"_\" + contact_cnt[\"A_ID\"].astype(str)\n",
      "C:\\Users\\Shannon\\AppData\\Local\\Temp\\ipykernel_23384\\2055995018.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  contact_cnt[\"B\"] = contact_cnt[\"orgB\"] +\"_\" + contact_cnt[\"B_ID\"].astype(str)\n",
      "C:\\Users\\Shannon\\AppData\\Local\\Temp\\ipykernel_23384\\2055995018.py:260: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cont_cnt[[col.split('_')[0] for col in org_df.columns if col.endswith((\"_count\"))]] = org_df[[col for col in org_df.columns if col.endswith((\"_count\"))]].astype(bool)\n",
      "C:\\Users\\Shannon\\AppData\\Local\\Temp\\ipykernel_23384\\2055995018.py:260: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cont_cnt[[col.split('_')[0] for col in org_df.columns if col.endswith((\"_count\"))]] = org_df[[col for col in org_df.columns if col.endswith((\"_count\"))]].astype(bool)\n",
      "C:\\Users\\Shannon\\AppData\\Local\\Temp\\ipykernel_23384\\2055995018.py:260: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cont_cnt[[col.split('_')[0] for col in org_df.columns if col.endswith((\"_count\"))]] = org_df[[col for col in org_df.columns if col.endswith((\"_count\"))]].astype(bool)\n",
      "C:\\Users\\Shannon\\AppData\\Local\\Temp\\ipykernel_23384\\2055995018.py:260: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cont_cnt[[col.split('_')[0] for col in org_df.columns if col.endswith((\"_count\"))]] = org_df[[col for col in org_df.columns if col.endswith((\"_count\"))]].astype(bool)\n",
      "C:\\Users\\Shannon\\AppData\\Local\\Temp\\ipykernel_23384\\2055995018.py:260: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cont_cnt[[col.split('_')[0] for col in org_df.columns if col.endswith((\"_count\"))]] = org_df[[col for col in org_df.columns if col.endswith((\"_count\"))]].astype(bool)\n",
      "C:\\Users\\Shannon\\AppData\\Local\\Temp\\ipykernel_23384\\2055995018.py:260: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cont_cnt[[col.split('_')[0] for col in org_df.columns if col.endswith((\"_count\"))]] = org_df[[col for col in org_df.columns if col.endswith((\"_count\"))]].astype(bool)\n",
      "C:\\Users\\Shannon\\AppData\\Local\\Temp\\ipykernel_23384\\2055995018.py:260: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cont_cnt[[col.split('_')[0] for col in org_df.columns if col.endswith((\"_count\"))]] = org_df[[col for col in org_df.columns if col.endswith((\"_count\"))]].astype(bool)\n",
      "C:\\Users\\Shannon\\AppData\\Local\\Temp\\ipykernel_23384\\2055995018.py:260: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cont_cnt[[col.split('_')[0] for col in org_df.columns if col.endswith((\"_count\"))]] = org_df[[col for col in org_df.columns if col.endswith((\"_count\"))]].astype(bool)\n",
      "C:\\Users\\Shannon\\AppData\\Local\\Temp\\ipykernel_23384\\2055995018.py:260: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cont_cnt[[col.split('_')[0] for col in org_df.columns if col.endswith((\"_count\"))]] = org_df[[col for col in org_df.columns if col.endswith((\"_count\"))]].astype(bool)\n",
      "C:\\Users\\Shannon\\AppData\\Local\\Temp\\ipykernel_23384\\2055995018.py:260: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cont_cnt[[col.split('_')[0] for col in org_df.columns if col.endswith((\"_count\"))]] = org_df[[col for col in org_df.columns if col.endswith((\"_count\"))]].astype(bool)\n",
      "C:\\Users\\Shannon\\AppData\\Local\\Temp\\ipykernel_23384\\2055995018.py:260: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cont_cnt[[col.split('_')[0] for col in org_df.columns if col.endswith((\"_count\"))]] = org_df[[col for col in org_df.columns if col.endswith((\"_count\"))]].astype(bool)\n",
      "C:\\Users\\Shannon\\AppData\\Local\\Temp\\ipykernel_23384\\2055995018.py:260: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cont_cnt[[col.split('_')[0] for col in org_df.columns if col.endswith((\"_count\"))]] = org_df[[col for col in org_df.columns if col.endswith((\"_count\"))]].astype(bool)\n",
      "C:\\Users\\Shannon\\AppData\\Local\\Temp\\ipykernel_23384\\2055995018.py:260: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cont_cnt[[col.split('_')[0] for col in org_df.columns if col.endswith((\"_count\"))]] = org_df[[col for col in org_df.columns if col.endswith((\"_count\"))]].astype(bool)\n",
      "C:\\Users\\Shannon\\AppData\\Local\\Temp\\ipykernel_23384\\2055995018.py:260: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cont_cnt[[col.split('_')[0] for col in org_df.columns if col.endswith((\"_count\"))]] = org_df[[col for col in org_df.columns if col.endswith((\"_count\"))]].astype(bool)\n",
      "C:\\Users\\Shannon\\AppData\\Local\\Temp\\ipykernel_23384\\2055995018.py:260: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cont_cnt[[col.split('_')[0] for col in org_df.columns if col.endswith((\"_count\"))]] = org_df[[col for col in org_df.columns if col.endswith((\"_count\"))]].astype(bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing of 4 files from 1 dataset(s) is complete.\n"
     ]
    }
   ],
   "source": [
    "out=_batch_summary_stats(csv_path_list=[\"C:/Users/zscoman/Documents/Python Scripts/Infer-subc-2D/data/test\"],\n",
    "                         out_path=\"C:/Users/zscoman/Documents/Python Scripts/Infer-subc-2D/data/test/sumstat\",\n",
    "                         out_preffix=\"20231117_prelim_\",\n",
    "                         splitter=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infer-subc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
